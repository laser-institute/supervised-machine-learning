---
title: "How Good is Our Model, Really?"
subtitle: "Conceptual Overview"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: img/LASERLogoB.png
    theme: [default, css/laser.scss]
    width: 1920
    height: 1080
    margin: 0.05
    footer: <a href=https://www.go.ncsu.edu/laser-institute>go.ncsu.edu/laser-institute
---

```{r}
# load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

# Purpose and Agenda

How do we interpret a machine learning model? What else can we say, besides how accurate a model is? This learning lab is intended to help you to answer these questions by examining output from a classification and a regression model. We continue working with the IPEDS data to understand and evaluate our predictions of institutional graduation rates.

## What we'll do in this presentation

- Discussion 1
- Key Concept: The journey from accuracy to comprehensive evaluation
- Key Concept: Understanding the confusion matrix
- Key Concept: Metrics and their real-world interpretation
- Key Concept: Feature engineering foundations
- Discussion 2
- Introduction to the other parts of this learning lab

# Discussion 1

::: {.panel-tabset}
## Background

- We are likely familiar with _accuracy_ and maybe another measure, _Cohen's Kappa_
- But, you may have heard of other means of determining how good a model is at making predictions: confusion matrices, specificity, sensitivity, recall, AUC-ROC, and others
- Broadly, these help us to understand _for which cases and types of cases a model is predictively better than others_ in a finer-grained way than accuracy

## Getting Started

- Think broadly and not formally (yet): What makes a prediction model a good one?

## Digging Deeper

- In the context of predicting institutional graduation rates, which do you think would be worse: falsely predicting an institution has good graduation rates when it doesn't, or falsely predicting it has poor graduation rates when it actually has good ones?

:::

# Key Concept #1: The journey from accuracy to comprehensive evaluation

## Our SML journey so far

<div style="display: flex; justify-content: space-between;">
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 1</h3>
  <p>Understanding prediction vs. explanation</p>
  <p><i>Same model, different goals</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 2</h3>
  <p>Implementing training/testing workflows</p>
  <p><i>Avoiding overfitting with proper evaluation</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #4a86e8; color: white; margin: 5px; border-radius: 10px;">
  <h3>Module 3</h3>
  <p>Evaluating model performance rigorously</p>
  <p><i>Beyond simple accuracy</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 4</h3>
  <p>Advanced feature engineering</p>
  <p><i>Improving models based on metrics</i></p>
</div>
</div>

## The limitation of accuracy

```{r, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
# Create sample data
set.seed(123)
model_a <- data.frame(
  actual = c(rep("Good", 85), rep("Poor", 15)),
  predicted = c(rep("Good", 70), rep("Poor", 15), rep("Good", 5), rep("Poor", 10))
)

model_b <- data.frame(
  actual = c(rep("Good", 15), rep("Poor", 85)),
  predicted = c(rep("Good", 10), rep("Poor", 5), rep("Good", 25), rep("Poor", 60))
)

# Calculate accuracy
accuracy_a <- sum(model_a$actual == model_a$predicted) / nrow(model_a)
accuracy_b <- sum(model_b$actual == model_b$predicted) / nrow(model_b)

# Display accuracies
results <- data.frame(
  Model = c("Model A", "Model B"),
  Accuracy = c(accuracy_a, accuracy_b),
  Class_Distribution = c("85% Good, 15% Poor", "15% Good, 85% Poor")
)

knitr::kable(results, digits = 2)
```

**Both models have 80% accuracy, but are they equally good?**

## Beyond accuracy: The need for nuance

- **Accuracy is insufficient when:**
  - Class distributions are imbalanced
  - Different types of errors have different consequences
  - You need to tune your model for specific objectives

- **Example from IPEDS data:**
  - What if only 20% of institutions have "good" graduation rates?
  - A model that always predicts "poor" would have 80% accuracy, but is useless for identifying successful institutions

- **We need metrics that tell us:**
  - How well we identify positive cases (good graduation rates)
  - How well we identify negative cases (poor graduation rates)
  - How reliable our positive and negative predictions are

# Key Concept #2: Understanding the confusion matrix

## The confusion matrix: Foundation of evaluation

```{r, echo=FALSE, fig.align="center", message=FALSE}
# Create example confusion matrix data
conf_matrix <- matrix(c(200, 50, 30, 120), nrow=2, 
                      dimnames=list(c("Predicted: Poor", "Predicted: Good"), 
                                   c("Actual: Poor", "Actual: Good")))

# Format as a table
knitr::kable(conf_matrix, caption="Example Confusion Matrix")
```

Every prediction falls into one of these four categories:

::: {.incremental}
- **True Negative (TN)**: Correctly predicted poor graduation rate (200)
- **False Positive (FP)**: Incorrectly predicted good graduation rate (50)
- **False Negative (FN)**: Incorrectly predicted poor graduation rate (30)
- **True Positive (TP)**: Correctly predicted good graduation rate (120)
:::

## Visualizing the confusion matrix

```{r, echo=FALSE, fig.width=10, fig.height=7, fig.align="center"}
library(ggplot2)
library(dplyr)

# Create data for visualization
cm_data <- data.frame(
  x = c("Actual: Poor", "Actual: Poor", "Actual: Good", "Actual: Good"),
  y = c("Predicted: Poor", "Predicted: Good", "Predicted: Poor", "Predicted: Good"),
  count = c(200, 50, 30, 120),
  label = c("True Negative\n(TN)", "False Positive\n(FP)", "False Negative\n(FN)", "True Positive\n(TP)"),
  category = c("Correct", "Error", "Error", "Correct")
)

# Create the heatmap
ggplot(cm_data, aes(x=x, y=y, fill=category)) +
  geom_tile(color="white", size=1.5) +
  geom_text(aes(label=paste0(label, "\n", count)), size=6) +
  scale_fill_manual(values=c("Correct"="#7DCEA0", "Error"="#F1948A")) +
  theme_minimal(base_size=16) +
  theme(axis.title=element_blank(),
        legend.position="none",
        plot.title=element_text(hjust=0.5, size=20, face="bold"),
        axis.text=element_text(size=16, face="bold")) +
  coord_fixed() +
  labs(title="Confusion Matrix: Institutional Graduation Rate Predictions")
```

## Confusion matrix: Real-world implications

Each cell in the confusion matrix has different consequences:

::: {.incremental}
- **True Negative**: Correctly identifying institutions with poor graduation rates
  - *Benefit*: Can target resources toward institutions needing improvement

- **False Positive**: Incorrectly labeling an institution as having good graduation rates
  - *Risk*: May overlook institutions that need intervention

- **False Negative**: Incorrectly labeling an institution as having poor graduation rates
  - *Risk*: May allocate resources unnecessarily or affect institutional reputation

- **True Positive**: Correctly identifying institutions with good graduation rates
  - *Benefit*: Can study these institutions as best practice exemplars
:::

# Key Concept #3: Metrics and their real-world interpretation

## From confusion matrix to metrics

```{r, echo=FALSE, fig.align="center", message=FALSE}
# Create metrics data
metrics <- data.frame(
  Metric = c("Accuracy", "Sensitivity/Recall", "Specificity", "Precision/PPV", "NPV", "F1 Score"),
  Formula = c("(TP + TN) / (TP + TN + FP + FN)", 
              "TP / (TP + FN)", 
              "TN / (TN + FP)", 
              "TP / (TP + FP)",
              "TN / (TN + FN)",
              "2 * (Precision * Recall) / (Precision + Recall)"),
  Question = c("Overall, how often is the model correct?",
               "When an institution actually has good graduation rates, how often does the model predict this?",
               "When an institution actually has poor graduation rates, how often does the model predict this?",
               "When the model predicts good graduation rates, how often is it correct?",
               "When the model predicts poor graduation rates, how often is it correct?",
               "What is the harmonic mean of precision and recall?"),
  Value = c("80%", "80%", "80%", "71%", "87%", "75%")
)

knitr::kable(metrics)
```

## Sensitivity vs. Specificity

```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align="center"}
# Create data for visualization
sensitivity_data <- data.frame(
  Category = c(rep("Actual: Good", 150)),
  Prediction = c(rep("True Positive", 120), rep("False Negative", 30)),
  Type = c(rep("Correct", 120), rep("Missed", 30))
)

specificity_data <- data.frame(
  Category = c(rep("Actual: Poor", 250)),
  Prediction = c(rep("True Negative", 200), rep("False Positive", 50)),
  Type = c(rep("Correct", 200), rep("Missed", 50))
)

# Create the sensitivity plot
p1 <- ggplot(sensitivity_data, aes(x=Category, fill=Type)) +
  geom_bar() +
  scale_fill_manual(values=c("Correct"="#7DCEA0", "Missed"="#F1948A")) +
  theme_minimal(base_size=14) +
  labs(title="Sensitivity: 80%",
       subtitle="Among institutions with good graduation rates,\nhow many did we correctly identify?",
       y="Count") +
  theme(legend.position="bottom",
        plot.title=element_text(hjust=0.5, size=16, face="bold"),
        plot.subtitle=element_text(hjust=0.5))

# Create the specificity plot
p2 <- ggplot(specificity_data, aes(x=Category, fill=Type)) +
  geom_bar() +
  scale_fill_manual(values=c("Correct"="#7DCEA0", "Missed"="#F1948A")) +
  theme_minimal(base_size=14) +
  labs(title="Specificity: 80%",
       subtitle="Among institutions with poor graduation rates,\nhow many did we correctly identify?",
       y="Count") +
  theme(legend.position="bottom",
        plot.title=element_text(hjust=0.5, size=16, face="bold"),
        plot.subtitle=element_text(hjust=0.5))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, ncol=2)
```

## Precision vs. Recall (Sensitivity)

```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align="center"}
# Create data for visualization
precision_data <- data.frame(
  Category = c(rep("Predicted: Good", 170)),
  Actual = c(rep("True Positive", 120), rep("False Positive", 50)),
  Type = c(rep("Correct", 120), rep("Incorrect", 50))
)

recall_data <- sensitivity_data
recall_data$Type <- ifelse(recall_data$Type == "Missed", "Incorrect", recall_data$Type)

# Create the precision plot
p1 <- ggplot(precision_data, aes(x=Category, fill=Type)) +
  geom_bar() +
  scale_fill_manual(values=c("Correct"="#7DCEA0", "Incorrect"="#F1948A")) +
  theme_minimal(base_size=14) +
  labs(title="Precision (PPV): 71%",
       subtitle="When we predict good graduation rates,\nhow often are we correct?",
       y="Count") +
  theme(legend.position="bottom",
        plot.title=element_text(hjust=0.5, size=16, face="bold"),
        plot.subtitle=element_text(hjust=0.5))

# Create the recall plot
p2 <- ggplot(recall_data, aes(x=Category, fill=Type)) +
  geom_bar() +
  scale_fill_manual(values=c("Correct"="#7DCEA0", "Incorrect"="#F1948A")) +
  theme_minimal(base_size=14) +
  labs(title="Recall (Sensitivity): 80%",
       subtitle="Among institutions with good graduation rates,\nhow many did we correctly identify?",
       y="Count") +
  theme(legend.position="bottom",
        plot.title=element_text(hjust=0.5, size=16, face="bold"),
        plot.subtitle=element_text(hjust=0.5))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, ncol=2)
```

## Metrics in context: What matters for IPEDS data?

When deciding which metrics to prioritize for institutional graduation rate prediction:

::: {.incremental}
- **High precision (PPV)** is important if:
  - You want to reliably identify exemplar institutions
  - You're allocating limited resources for site visits or case studies
  - False positives would be costly (e.g., wrongly recognizing institutions)

- **High recall (sensitivity)** is important if:
  - You don't want to miss any high-performing institutions
  - You're creating a comprehensive database of successful models
  - The cost of false negatives is high (e.g., overlooking innovations)

- **High specificity** is important if:
  - You want to accurately identify institutions needing intervention
  - You're allocating improvement resources
  - You need to minimize false alarms about poor performance
:::

## The precision-recall tradeoff

```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align="center", message=FALSE, warning=FALSE}
# Create data for ROC curve
set.seed(123)
n <- 500
true_prob <- c(rbeta(n/2, 2, 5), rbeta(n/2, 5, 2))
actual <- ifelse(true_prob > 0.5, 1, 0)
noise <- rnorm(n, 0, 0.2)
pred_prob <- pmin(pmax(true_prob + noise, 0), 1)

results <- data.frame(actual = factor(actual), 
                     pred_prob = pred_prob)

# Calculate metrics at different thresholds
thresholds <- seq(0.1, 0.9, by=0.1)
pr_data <- data.frame()

for(t in thresholds) {
  results$predicted <- factor(ifelse(results$pred_prob > t, 1, 0))
  
  # Calculate confusion matrix values
  cm <- table(predicted = results$predicted, actual = results$actual)
  if(nrow(cm) < 2 || ncol(cm) < 2) next
  
  TP <- cm[2,2]
  FP <- cm[2,1]
  FN <- cm[1,2]
  TN <- cm[1,1]
  
  # Calculate metrics
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  
  pr_data <- rbind(pr_data, data.frame(
    threshold = t,
    precision = precision,
    recall = recall
  ))
}

# Create precision-recall curve
ggplot(pr_data, aes(x=recall, y=precision)) +
  geom_line(size=1.5, color="#3498DB") +
  geom_point(aes(color=threshold), size=4) +
  scale_color_viridis_c() +
  theme_minimal(base_size=14) +
  labs(title="Precision-Recall Tradeoff",
       x="Recall (Sensitivity)",
       y="Precision",
       color="Threshold") +
  theme(plot.title=element_text(hjust=0.5, size=18, face="bold"),
        legend.position="right") +
  annotate("text", x=0.7, y=0.65, 
           label="Each point represents a different\nclassification threshold", 
           size=5, hjust=0) +
  annotate("segment", x=0.67, y=0.65, xend=0.62, yend=0.7, 
           arrow=arrow(length=unit(0.3,"cm")), size=1)
```

## The ROC curve and AUC

```{r, echo=FALSE, fig.width=10, fig.height=6, fig.align="center", message=FALSE, warning=FALSE}
# Calculate metrics for ROC curve
roc_data <- data.frame()

for(t in seq(0, 1, by=0.01)) {
  results$predicted <- factor(ifelse(results$pred_prob > t, 1, 0))
  
  # Calculate confusion matrix values
  cm <- table(predicted = results$predicted, actual = results$actual)
  if(nrow(cm) < 2 || ncol(cm) < 2) {
    # Handle edge cases where the confusion matrix doesn't have all cells
    if(t <= 0.01) {
      tpr <- 1
      fpr <- 1
    } else {
      tpr <- 0
      fpr <- 0
    }
  } else {
    TP <- cm[2,2]
    FP <- cm[2,1]
    FN <- cm[1,2]
    TN <- cm[1,1]
    
    # Calculate metrics
    tpr <- TP / (TP + FN)  # True Positive Rate (Sensitivity)
    fpr <- FP / (FP + TN)  # False Positive Rate (1-Specificity)
  }
  
  roc_data <- rbind(roc_data, data.frame(
    threshold = t,
    tpr = tpr,
    fpr = fpr
  ))
}

# Calculate AUC using trapezoidal rule
roc_data <- roc_data[order(roc_data$fpr), ]
auc_value <- 0
for(i in 2:nrow(roc_data)) {
  auc_value <- auc_value + (roc_data$fpr[i] - roc_data$fpr[i-1]) * (roc_data$tpr[i] + roc_data$tpr[i-1]) / 2
}

# Create ROC curve
ggplot(roc_data, aes(x=fpr, y=tpr)) +
  geom_abline(intercept=0, slope=1, linetype="dashed", color="gray50") +
  geom_line(size=1.5, color="#E74C3C") +
  geom_point(data=roc_data[seq(1, nrow(roc_data), 10), ], aes(color=threshold), size=3) +
  scale_color_viridis_c() +
  theme_minimal(base_size=14) +
  labs(title=paste("ROC Curve (AUC =", round(auc_value, 2), ")"),
       x="False Positive Rate (1-Specificity)",
       y="True Positive Rate (Sensitivity)",
       color="Threshold") +
  theme(plot.title=element_text(hjust=0.5, size=18, face="bold"),
        legend.position="right") +
  annotate("text", x=0.75, y=0.25, 
           label="Diagonal line represents\na random classifier (AUC = 0.5)", 
           size=5, hjust=0) +
  annotate("text", x=0.25, y=0.75, 
           label="Better models push toward\nthe upper-left corner", 
           size=5, hjust=0) +
  annotate("segment", x=0.25, y=0.73, xend=0.15, yend=0.85, 
           arrow=arrow(length=unit(0.3,"cm")), size=1)
```

## Choosing the right classification threshold

The choice of threshold depends on your specific needs and priorities:

::: {.incremental}
- **Default threshold (0.5)**: Balanced, but often not optimal
  
- **Lower threshold**: Increases sensitivity, decreases specificity
  - *Pro*: Identifies more institutions with good graduation rates
  - *Con*: More false positives (incorrectly labeled as good)
  
- **Higher threshold**: Increases specificity, decreases sensitivity
  - *Pro*: More confident in positive predictions (high precision)
  - *Con*: Misses more institutions with good graduation rates
  
- **Optimal threshold depends on:**
  - The relative costs of false positives vs. false negatives
  - Available resources for intervention
  - Stakeholder priorities
:::

# Key Concept #4: Feature engineering foundations

## What is feature engineering?

::: {.incremental}
- Feature engineering is the process of creating or transforming variables to improve model performance
  
- Good features:
  - Capture meaningful patterns in the data
  - Relate to the underlying phenomenon (graduation rates)
  - Help distinguish between outcome classes
  
- For our IPEDS data, features might include:
  - Ratios (e.g., faculty-to-student ratio)
  - Derived variables (e.g., regional indicators)
  - Transformed variables (e.g., log of enrollment)
:::

## Selecting metrics to guide feature engineering

The metrics you prioritize should guide your feature engineering efforts:

::: {.incremental}
- **If precision is low:** Engineer features that help reduce false positives
  - Example: Create features that better distinguish truly high-performing institutions
  
- **If recall is low:** Engineer features that help reduce false negatives
  - Example: Create features that capture different aspects of institutional success
  
- **If both are problematic:** Consider:
  - More sophisticated models (Module 4)
  - Additional data sources
  - Domain-specific transformations
:::

## Connecting metrics to feature engineering

```{r, echo=FALSE, fig.align="center", message=FALSE}
# Create feature engineering suggestions based on metrics
suggestions <- data.frame(
  Metric_Issue = c("Low Precision", "Low Sensitivity/Recall", "Low Specificity", "Overall Low Performance"),
  Feature_Engineering_Strategy = c(
    "Create features that better separate true positives from false positives (e.g., faculty quality metrics, program-specific graduation rates)",
    "Develop features that capture alternative indicators of success (e.g., post-graduation employment, student satisfaction)",
    "Focus on features that identify struggling institutions (e.g., student support resources, financial stability indicators)",
    "Incorporate domain knowledge to create transformed variables, interaction terms, and polynomial features"
  ),
  Example_for_IPEDS = c(
    "Ratio of instructional budget to administrative expenses",
    "Percentage of students in high-demand majors or programs",
    "Student support services per enrolled student",
    "Interaction between admission selectivity and financial aid availability"
  )
)

knitr::kable(suggestions)
```

# Discussion 2

::: {.panel-tabset}
## Reflecting

- Which metrics for supervised machine learning models (in classification "mode") do you think would be most important when predicting institutional graduation rates? Why?

## Applying

- Think about a specific educational prediction problem in your research. Which evaluation metrics would be most important for that problem, and why?

:::

# Introduction to the other parts of this learning lab

::: {.panel-tabset}

## Readings

> Baker, R. S., Berning, A. W., Gowda, S. M., Zhang, S., & Hawn, A. (2020). Predicting K-12 dropout. Journal of Education for Students Placed at Risk (JESPAR), 25(1), 28-54.

> Baker, R. S., Bosch, N., Hutt, S., Zambrano, A. F., & Bowers, A. J. (2024). On fixing the right problems in predictive analytics: AUC is not the problem. arXiv preprint. https://arxiv.org/pdf/2404.06989

> Maestrales, S., Zhai, X., Touitou, I., Baker, Q., Schneider, B., & Krajcik, J. (2021). Using machine learning to score multi-dimensional assessments of chemistry and physics. Journal of Science Education and Technology, 30(2), 239-254.

## Case study

- Adding more sophisticated metrics to evaluate our IPEDS graduation rate predictions
- Building on the features we developed in Module 2
- Understanding which metrics are most appropriate for our particular prediction context
- Learning to use the `metric_set()` function from tidymodels

## Badge

- Applying metrics to a regression modeling task
- Extending our model with additional predictors
- Interpreting metrics in the context of continuous outcomes
- Reflecting on the appropriateness of different modeling approaches

:::

# *fin*

*Dr. Joshua Rosenberg* (jmrosenberg@utk.edu; https://joshuamrosenberg.com)

[General troubleshooting tips for R and RStudio](https://docs.google.com/document/d/14Jc-KG3m5k1BvyKWqw7KmDD21IugU5nV5edfJkZyspY/edit)