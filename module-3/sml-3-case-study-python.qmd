---
title: "Beyond Accuracy: How to Calculate Metrics"
subtitle: "Case Study Key"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
resource_files:
- img/tidymodels.png
---

```{r}
#| include: false
reticulate::py_install("pandas")
reticulate::py_install("numpy")
reticulate::py_install("statsmodels")
reticulate::py_install("scikit-learn")
```

## 1. PREPARE

When we left off the first case study, we saw that our model was *pretty accurate*. We can and will do better in terms of accuracy. But, the accuracy value we found also raises a broader question: How good was our model in terms of making predictions?

While accuracy is an easy to understand and interpret value, it provides a limited answer to the above question about how good our model was in terms of making predictions.

In this learning and case study, we explore a variety of ways to understand how good of a predictive model ours is. We do this through a variety of means:

-   Other **statistics**, such as sensitivity and specificity

-   **Tables**--particularly, a confusion matrix

Our driving question for this case study, then, is: *How good is our model at making predictions?*

We'll use the [Open University Learning Analytics Dataset (OULAD)](https://analyse.kmi.open.ac.uk/open_dataset).

The OULAD was created by learning analytics researchers at the United Kingdom-based Open University. It includes data from post-secondary learners participation in one of several Massive Open Online Courses (called *modules* in the OULAD).

> Kuzilek, J., Hlosta, M., & Zdrahal, Z. (2017). Open university learning analytics dataset. *Scientific Data, 4(*1), 1-8. <https://www.nature.com/articles/sdata2017171>
>
> **Abstract:** Learning Analytics focuses on the collection and analysis of learners' data to improve their learning experience by providing informed guidance and to optimise learning materials. To support the research in this area we have developed a dataset, containing data from courses presented at the Open University (OU). What makes the dataset unique is the fact that it contains demographic data together with aggregated clickstream data of students' interactions in the Virtual Learning Environment (VLE). This enables the analysis of student behavior, represented by their actions. The dataset contains the information about 22 courses, 32,593 students, their assessment results, and logs of their interactions with the VLE represented by daily summaries of student clicks (10,655,280 entries). The dataset is freely available at <https://analyse.kmi.open.ac.uk/open_dataset> under a CC-BY 4.0 license.

#### **ðŸ‘‰ Your Turn** **â¤µ**

You don't need to read the entire article yet, but please open this article, scan the sections, and write down two things you notice or wonder about the dataset.

1.  YOUR RESPONSE HERE

2.  YOUR RESPONSE HERE

Like in the first learning module, we'll first load several packages -- pandas and numpy. Note that if they're already installed, you don't need to install them again.

```{python}
import pandas as pd 
import numpy as np 
```

We will also import a number of scikit-learn packages. Some are used in previous modules, but some we will introduce in this case study:

```{python}
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, cohen_kappa_score

# These are new
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
```

## 2. WRANGLE

Like in the code-along, let's take a look at the data and do some processing of it.

#### **ðŸ‘‰ Your Turn** **â¤µ**

We'll load the students file together; you'll write code to read the assessments file, which is named "oulad-assessments.csv". Please assign the name `assessments` to the loaded assessments file.

```{python}

```

## 3. EXPLORE

#### **ðŸ‘‰ Your Turn** **â¤µ**

In the last learning module, we used the `value_counts()` function. Let's practice that again, by counting the number of of assessments of different types. If you need, recall that the data dictionary is [here](https://analyse.kmi.open.ac.uk/open_dataset). Note what the different types of assessments mean.

```{python}

```

-   

#### **ðŸ‘‰ Your Turn** **â¤µ**

We'll now use another function--like `value_counts()`, from the pandas library. Specifically, we'll use the `nunique()` function. This returns the number of unique values for a specified variable. Learn more about `nunique` [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html). Below, find the distinct assessment IDs.

```{python}

```

Let's explore the assessments data a bit.

We might be interested in how many assessments there are per course. We can use the pandas `groupby()` and `size()` to make a grouped pandas DataFrame:

```{python}
assessment_count_df = (
    assessments_df
        .groupby(['assessment_type', 'code_module', 'code_presentation'])
        .size()
        .reset_index(name='n')
)

assessment_count_df.head()
```

Let's explore the dates assignments were submitted a bit -- using the `describe()` method:

```{python}
assessments_df[['date']].describe()
```

What can we take from this? It looks like, on average, the average (mean and median) date assignments were due was around 130 -- 130 days after the start of the course. The first assignment seems to have been due 12 days into the course, and the last 261 days after.

Crucially, though, these dates vary by course. Thus, we need to first group the data by course. Let's use a different function this time -- `quantile()`, and calculate the first quantile value. Our reasoning is that we want to be able to act to support students, and if we wait until after the average assignment is due, then that might be too late. Whereas the first quantile comes approximately one-quarter through the semester --- with, therefore, more time to intervene.

Alright, this is a bit complicated, but we can actually work with this data. Let's use the `groupby` and `quantile` function on the `date` column, assigning it the name `code_module_dates`.

```{python}
code_module_dates_df = (
    assessments_df
    .groupby(['code_module', 'code_presentation'])['date']
    .quantile(0.25)
    .reset_index(name='quantile_cutoff_date')
)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Let's take a look at what we just created; type `code_module_dates` below:

```{python}

```

What have we created? We found the date that is one-quarter of the way through the semester (in terms of the dates assignments are due).

#### **ðŸ‘‰ Your Turn** **â¤µ**

We can thus use this to group and calculate students' performance on assignments through this point. To do this, we need to use a join function --- The pandas `merge()`, in particular (documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.merge.html)).

Please use the pandas merge function to join together `assessments_df` and `code_module_dates_df`, with `assessments_df` being the "left" data frame, and `code_module_dates_df` the "right". To specify the type of join, set the `how` parameter to 'left' and the `on` parameter to `['code_module', 'code_presentation']`.

Save the output of the join the name `assessments_joined_df`.

```{python}

```

We're almost there! The next few line filters the assessments data so it only includes assessments before our cutoff date.

```{python}
assessments_filtered_df = assessments_joined_df.query('date < quantile_cutoff_date') # filter the data so only assignments before the cutoff date are included
```

Finally, we'll find the average score for each student.

```{python}
assessments_filtered_df['weighted_score'] = assessments_filtered_df['score'] * assessments_filtered_df['weight']  # create a new variable that accounts for the "weight" (comparable to points) given each assignment

assessments_summarized_df = (
    assessments_filtered_df
    .groupby('id_student')['weighted_score']
    .mean()
    .reset_index(name='mean_weighted_score')
)
```

As a point of reflection here, note how much work we've done before we've gotten to the machine learning steps. Though a challenge, this is typical for both machine learning and more traditional statistical models: a lot of the work is in the preparation and data wrangling stages.

Let's copy the code below that we used to process the students data (when processing the `pass` and `imd_band` variables).

```{python}
students_df['pass'] = np.where(students_df['final_result'] == 'Pass', 1, 0)

imd_mapping = {  # maps from the text to a numerical, ordinal value (aka category 4 less than 5 and 5 represents values less than 6)
    "0-10%": 1,
    "10-20%": 2,
    "20-30%": 3,
    "30-40%": 4,
    "40-50%": 5,
    "50-60%": 6,
    "60-70%": 7,
    "70-80%": 8,
    "80-90%": 9,
    "90-100%": 10
}

students_df['imd_band'] = students_df['imd_band'].map(imd_mapping)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Finally, let's join together `students_df` and `assessments_summarized_df` on the id_student column, assigning the joined the name `students_and_assessments_df`.

```{python}

```

Big picture, we've added another measure -- another feature -- that we can use to make predictions: students' performance on assessments 1/4 of the way through the course.

We're now ready to proceed to our machine learning steps!

The problem we will be working on - predicting students who pass, based on data from the first one-third of the semester - has an analog in a recent paper by Ryan Baker and colleagues:

In [Baker et al. (2020)](https://www.tandfonline.com/doi/full/10.1080/10824669.2019.1670065), the authors create a precision-recall (also known as sensitivity) graph - one that demonstrates the trade-off between optimizing these two statistics. Review their paper - especially the results section - to see how they discuss these two statistics.

> Baker, R. S., Berning, A. W., Gowda, S. M., Zhang, S., & Hawn, A. (2020). Predicting K-12 dropout. Journal of Education for Students Placed at Risk (JESPAR), 25(1), 28-54.

Please review this paper before proceeding, focusing on how they describe

## 4. MODEL

### Step 1. Split data

This is identical to what we did in the first learning module, using the `students_and_assessments` data. We'll also create a testing data set we'll use later.

```{python}

students_and_assessments_df = students_and_assessments_df.dropna(subset=['mean_weighted_score'])

train_df, test_df = train_test_split(
    students_and_assessments_df, 
    test_size=0.5, 
    stratify=students_and_assessments_df['pass'], 
    random_state=20240712,
)
```

One we have a train and test dataset, we'll split apart the independent (`X`) from the dependent (`y`) variables.

```{python}
X_train = train_df.drop(columns=['pass'])  # Keeps all columns except our outcome column 'pass'
y_train = train_df['pass']  # Isolates just the outcome column

X_test = test_df.drop(columns=['pass'])
y_test = test_df['pass']
```

### Step 2: Build the modeling Pipeline:

Real-world datasets often include a mix of categorical variables (such as a `gender` column with values like *Male*, *Female*, or *Non-binary*) and numerical variables (like average test scores; e.g., `mean_weighted_score`). These two types of data require different preprocessing steps before they can be used in a machine learning model. For instance, missing values in numerical features are often filled using the columnâ€™s median, while missing values in categorical features are typically replaced with the most frequent category.

In this section, we introduce a new preprocessing technique using scikit-learnâ€™s `OneHotEncoder`. This tool transforms a single categorical column into multiple binary columns, one for each unique category. This step ("one-hot encoding") is necessary because most machine learning models only accept numerical input. Some libraries, such as the *statsmodels* library we've used in earlier exercises, automatically handle categorical variables behind the scenes. However, with scikit-learn, we apply one-hot encoding manually, which gives us greater control over how the data is transformed and ensures consistency throughout the pipeline.

```{python}
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(drop="first", handle_unknown="ignore")),
])

numerical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
])

```

Scikit-learnâ€™s `ColumnTransformer` lets us apply different preprocessing steps to different columns in a single, organized pipeline. This is especially useful for datasets with both numerical and categorical features, allowing us to handle each type appropriately by referring to column names directly.

We can then combine the `ColumnTransformer` with a modelâ€”such as `LogisticRegression`â€”using a `Pipeline`. This bundles preprocessing and modeling into one object, making it easy to train the model and make predictions on new data, while keeping the code clean and consistent.

```{python}
categorical_features = [
    'disability',
    'gender',
    'code_module',
]

numerical_features = [
    'date_registration',
    'mean_weighted_score',
]

preprocessor = ColumnTransformer(transformers=[
    ("cat", categorical_transformer, categorical_features),  
    ("num", numerical_transformer, numerical_features)
])

# Full pipeline
model_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression())
])
```

In this example, we refer to columns by name. However, when applying transformations to all columns of a certain type, a common approach is to use pandas' `select_dtypes()` method to automatically detect and group them by data type.

```{python}
# This is example code:
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()
```

### Step 3: Fit model and calculate metrics

Finally, we'll use the `fit` function on the `model_pipeline` object to train the model with the training dataset, and the `predict` method to make new predictions on the test dataset.

#### **ðŸ‘‰ Your Turn** **â¤µ**

```{python}

```

Once youâ€™ve generated predictions on the test dataset, follow the same pattern we used in the earlier code-along to calculate the full set of evaluation metrics. These include: accuracy, sensitivity (recall), specificity, precision, negative predictive value, and Cohenâ€™s kappa.

```{python}

```

We're now ready to move on to interpreting the accuracy of our model.

### Step 4: Interpret accuracy

So, what's the accuracy? Add that below as a percentage.

-   **Accuracy:** 56.6%

### Other measures of predictive accuracy

Here's where things get interesting: There are other statistics that have different denominators. Recall from the overview presentation that we can slice and dice the confusion matrix to calculate statistics that give us insights into the predictive utility of the model. Based on the above **Values** for TP, TN, FP, and FN you interpreted a few moments ago, add the **Statistic Values** for sensitivity, specificity, precision, and negative predictive value below to three decimal points.

#### **ðŸ‘‰ Your Turn** **â¤µ**

|  |  |  |  |  |
|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|
| **Statistic** | **Equation** | **Interpretation** | **Question Answered** | **Statistic Values** |
| **Sensitivity** (AKA recall) | TP / (TP + FN) | Proportion of those who are affected by a condition and correctly tested positive |  |  |
| **Specificity** | TN / (FP + TN) | Proportion of those who are not affected by a condition and correctly tested negative; |  |  |
| **Precision** (AKA Positive Predictive Value) | TP / (TP + FP) | Proportion of those who tested positive who are affected by a condition |  |  |
| **Negative Predictive Value** | TN / (TN + FN) | Proportion of those who tested positive who are not affected by a condition; *the probability that a negative test is correct* |  |  |

So, what does this hard-won by output tell us? Let's interpret each statistic carefully in the table below. Please add the value and provide a *substantive interpretation*. One is provided to get you started.

#### **ðŸ‘‰ Your Turn** **â¤µ**

|  |  |
|------------------------------------|------------------------------------|
| **Statistic** | **Substantive Interpretation** |
| Accuracy |  |
| Sensitivity (AKA recall) | The model correctly predicts about 2/3 of students who do not pass correctly (as not passing). This is pretty good, but it could be better. |
| Specificity |  |
| Precision (AKA Positive Predictive Value) |  |
| Negative Predictive Value |  |

This process might suggest to you how a "good" model isn't necessarily one that is the most accurate, but instead is one that has good values for statistics that matter given our particular question and context.

Recall that Baker and colleagues sought to balance between precision and recall (specificity). Please briefly discuss how well our model does this; is it better suited to correctly identifying "positive" pass cases (sensitivity) or "negatively" identifying students who do not pass (specificity)?

-   

## 5. COMMUNICATE

### Quickly calculating metrics

That was quite a bit of work to calculate those metrics manually. If evaluating models in this way is something we plan to do frequently, it makes sense to encapsulate the logic in a reusable function. This not only saves time and reduces repetitive code, but also helps ensure **consistency** across experiments and makes our analysis easier to maintain and debug as our projects grow.

```{python}
def calculate_model_performance_metrics(y_actual, y_preds) -> dict: 
    """
    Calculates a suite of classification performance metrics.

    This function computes common evaluation metrics for binary classification,
    including accuracy, sensitivity (recall), specificity, precision,
    negative predictive value, and Cohenâ€™s kappa. It assumes the positive class is labeled as 1.

    Args:
        y_actual (list or array): Ground truth (true) binary labels.
        y_preds (list or array): Predicted binary labels.

    Returns:
        dict: A dictionary containing all metrics:
    """
    accuracy = accuracy_score(y_actual, y_preds)
    precision = precision_score(y_actual, y_preds, pos_label=1)
    kappa = cohen_kappa_score(y_actual, y_preds)
    
    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_actual, y_preds).ravel()
    
    sensitivity = true_positive / (true_positive + false_negative)  # or the sklearn.metrics recall_score function
    specificity = true_negative / (true_negative + false_positive)
    
    negative_predictive_value = true_negative / (true_negative + false_negative)

    return {
        "accuracy": accuracy,
        "sensitivity": sensitivity,
        "specificity": specificity,
        "precision": precision,
        "negative_predictive_value": negative_predictive_value,
        "cohen_kappa": kappa
    }
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Use the `calculate_model_performance_metrics` function defined above to calculate the suite of metrics.

```{python}

```

Having invested in understanding the terminology of machine learning metrics, we'll use this "shortcut" going forward.

### ðŸ§¶ Knit & Check âœ…

Congratulations - you've completed this case study! Consider moving on to the badge activity next.