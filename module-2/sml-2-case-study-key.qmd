---
title: "Predicting Institutional Graduation Rates Using Testing and Training Data"
subtitle: "Case Study"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
---

## 1. PREPARE

We'll focus on the core parts of doing a machine learning analysis in R, this time bringing into the picture the key concept of training and testing data, which we deliberately omitted from the last case study. We'll use the {[tidymodels](https://www.tidymodels.org/)} set of R packages (add-ons) to do so, like in the first module. However, to help anchor our analysis and provide us with some direction, we'll focus on the following research question as we explore this new approach:

> How well can we predict which institutions have higher graduation rates?

This builds directly on the work from Module 1, where we started investigating this question. In this module, we'll properly implement a training and testing workflow to evaluate our predictions more rigorously. We'll also investigate our data set further to ensure that we understand what goes into the model!

#### Reading: Statistical modeling: The two cultures

> Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). *Statistical Science, 16*(3), 199-231. <https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf>

**ðŸ‘‰ Your Turn** **â¤µ**

You'll be asked to reflect more deeply on this article later on (in the badge activity); but for now, open up the article and take a quick scan of the article and note below an observation or question you have about the article.

-   YOUR RESPONSE HERE

#### Reading: Predicting students' final grades

> Estrellado, R. A., Freer, E. A., Mostipak, J., Rosenberg, J. M., & VelÃ¡squez, I. C. (2020). *Data science in education using R*. Routledge (c14), Predicting students' final grades using machine learning methods with online course data. <http://www.datascienceineducation.com/>

Please review this chapter, focusing on the overall goals of the analysis and how the analysis was presented (focusing on predictions, rather than the ways we may typically interpret a statistical model--like measures of statistical significance).

### 1b. Load Packages

Like in the last module, please load the tidyverse package. Also, please load the tidymodels and janitor packages in the code chunk below. Also, add another package we'll use, skimr. Install it if you need.

```{r load-packages}
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
```

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled (Krumm et al, 2018). In Part 2, we focus on the following wrangling processes to:

1.  **Importing and Inspecting Data**. In this section, we will "read" in our CSV data file and take a quick look at what our file contains.

2.  **Transform Variables**. We use the `mutate()` function to create a dichotomous variable for whether or not the institution has a "good" graduation rate, building on what we did in Module 1.

### 2a. Import and Inspect Data

For this module, we'll be working with the same IPEDS (Integrated Postsecondary Education Data System) dataset we used in Module 1. Let's read it in:

```{r}
ipeds <- read_csv("data/ipeds-all-title-9-2022-data.csv")
```

We'll use the same data cleaning steps from Module 1:

```{r}
ipeds <- clean_names(ipeds)
```

```{r}
ipeds <- ipeds %>% 
    select(name = institution_name, 
           title_iv = hd2022_postsecondary_and_title_iv_institution_indicator, # is the university a title IV university?
           carnegie_class = hd2022_carnegie_classification_2021_basic, # which carnegie classification
           state = hd2022_state_abbreviation, # state
           total_enroll = drvef2022_total_enrollment, # total enrollment
           pct_admitted = drvadm2022_percent_admitted_total, # percentage of applicants admitted
           n_bach = drvc2022_bachelors_degree, # number of students receiving a bachelor's degree
           n_mast = drvc2022_masters_degree, # number receiving a master's
           n_doc = drvc2022_doctors_degree_research_scholarship, # number receive a doctoral degree
           tuition_fees = drvic2022_tuition_and_fees_2021_22, # total cost of tuition and fees
           grad_rate = drvgr2022_graduation_rate_total_cohort, # graduation rate
           percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # percent of students receive financial aid
           avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks) # average salary of instructional staff
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Just as we did in Module 1, let's filter the data to include only Title IV postsecondary institutions.

```{r}
ipeds <- ipeds %>% 
    filter(title_iv == "Title IV postsecondary institution")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Now, filter the data again to *only* include institutions with a carnegie classification. Specifically, exclude those institutions with a value for the `carnegie_class` variable that is "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)". As a hint: whereas the logical operator `==` is used to include only matching conditions, the logical operator `!=` excludes matching conditions.

```{r}
ipeds <- ipeds %>% 
    filter(carnegie_class != "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Let's inspect our data - using `glimpse()` or another means of your choosing below.

```{r}
glimpse(ipeds)
```

Write down a few observations after inspecting the data:

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

### 2b. Transform Variables

We'll now transform our graduation rate variable into a binary outcome for our classification task, just as we did in Module 1.

#### **ðŸ‘‰ Your Turn** **â¤µ**

Create a binary variable called `good_grad_rate` that indicates whether an institution has a graduation rate above a certain threshold. Choose a threshold that seems reasonable to you.

```{r}
ipeds <- ipeds %>% 
    mutate(good_grad_rate = if_else(grad_rate > 62, 1, 0),
           good_grad_rate = as.factor(good_grad_rate))
```

Here, add a reason or two for how and why you picked the threshold you did:

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

## 3. EXPLORE

As noted by Krumm et al. (2018), exploratory data analysis often involves some combination of data visualization and *feature engineering*. In Part 3, we will create a quick visualization to help us spot any potential issues with our data and engineer new predictive variables or "features" that we will use in our predictive models.

### 3a. Examine Variables

Let's take a closer look at our institutional data. In the chunk below, count the number of institutions with good and poor graduation rates.

```{r}
ipeds %>% 
    count(good_grad_rate)
```

What does this tell us?

We can use the `tabyl` function to explore a bit further:

```{r}
ipeds %>% 
    tabyl(good_grad_rate)
```

We can see how many institutions have good and not-so-good graduations rates -- and what percentage of the data is missing.

Next, let's visualize the distribution of graduation rates to better understand our data:

```{r}
ipeds %>% 
    ggplot(aes(x = grad_rate)) +
    geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
    labs(title = "Distribution of Graduation Rates",
         x = "Graduation Rate (%)",
         y = "Number of Institutions")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Create another visualization that explores the relationship between two variables in our dataset. For example, you might want to look at the relationship between tuition fees and graduation rates, or enrollment and graduation rates.

```{r}
# Add your visualization code here

```

One last, critical step is understanding missingness in our data. The `skim()` function from the {skimr} package is fantastic for this --- it tells us the number and percentage of missing values for each variable in our data set.

```{r}
skim(ipeds)
```

From this, we can see that several variables have considerable missingness. For example, the `pct_admitted` variable has more than half of its values missing! There are ways to deal with this (real-life datasets, including the IPEDS data, are rife with missing values for a whole host of reasons). We will address those (e.g., imputation) later.

For this analysis, though, we'll focus on the variables that have relatively few missing values - `tuition_fees`, `percent_fin_aid`, `avg_salary`, and our dependent variable, `good_grad_rate`. This will be a simple model --- simpler in terms of the features than the model in the first analysis, but with the knowledge that we are using variables that are more complete.

### 3b. Feature Engineering

As defined by Krumm, Means, and Bienkowski (2018) in *Learning Analytics Goes to School*:

> **Feature engineering** is the process of creating new variables within a dataset, which goes above and beyond the work of recoding and rescaling variables.

The authors note that feature engineering draws on substantive knowledge from theory or practice, experience with a particular data system, and general experience in data-intensive research. Moreover, these features can be used not only in machine learning models, but also in visualizations and tables comprising descriptive statistics.

Though not as often discussed as other steps, feature engineering is an important step. You can read more about feature engineering [here](https://www.tmwr.org/recipes.html).

For this module, we'll create a new feature that might help predict graduation rates. Let's create a derived variable representing the ratio of tuition and fees to average salary, which could indicate the institution's value proposition.

```{r}
ipeds <- ipeds %>% 
    mutate(tuition_to_salary_ratio = tuition_fees / avg_salary)
```

Let's examine this new feature:

```{r}
ipeds %>% 
    ggplot(aes(x = tuition_to_salary_ratio, fill = good_grad_rate)) +
    geom_density()
```

Something is very not right. It seems like there may be some number that is way off -- and that's skewing the plot in a weird way.

What's going on? In this case, it can be helpful to get eyes on the data to see if anything funny seems to be going on. Let's take a look at the relevant variables:

```{r}
select(ipeds, name, tuition_fees, avg_salary, tuition_to_salary_ratio) %>% 
    arrange((desc(tuition_to_salary_ratio)))
```

Do you spot the problem? Add a note below about what it might be:

-   YOUR RESPONSE HERE

As you likely noticed, it looks like one institution - Beckfield College-Florence - has an average salary of \$1 for their graduates. As a result, the ratio of tuition and fees relative to salary is *huge* -- far larger than for any other institutions! What to do here? This seems almost certain to be an error in data reporting or recording. We can safely filter out this one row with some simple logic.

```{r}
ipeds <- ipeds %>% 
    filter(avg_salary > 1)
```

Let's try to create that plot again:

```{r}
ipeds %>% 
    ggplot(aes(x = tuition_to_salary_ratio, fill = good_grad_rate)) +
    geom_density()
```

That's better! Add a note on what this graph might mean below:

-   YOUR RESPONSE HERE

We're now ready to proceed to the five machine learning (modeling) steps!

## 4. MODEL

In this step, we will dive into the SML modeling process in much more depth than in the last module.

1.  **Split Data** into a training and test set that will be used to develop a predictive model;

2.  **Create a "Recipe"** for our predictive model and learn how to deal with nominal data that we would like to use as predictors;

3.  **Specify the model and workflow** by selecting the *functional form* of the model that we want and using a *model workflow* to pair our model and recipe together;

4.  **Fit Models** to our training set using logistic regression;

5.  **Interpret Accuracy** of our model to see how well our model can "predict" our outcome of interest.

### Step 1. Split data

We didn't split the data we used in the first module. Instead, we used data to *train* our model, and then we used the same data set to evaluate *how good our model performed*, i.e., to *test* our model. This raises an issue: using the same data to train and test our model can lead to something called overfitting, which is when a model learns the specifics in the training data rather than the underlying pattern.

This is related to the bias-variance trade off introduced in the conceptual overview: by using the same data to train and test our model, we run the risk of developing a model with low bias, but high variance. Any changes in the training data could result in a very different model (with far higher bias), which is not ideal. In other words, we want to develop a model that is generalizable to new data, not just the data we used to train it.

The authors of Data Science in Education Using R (Estrellado et al., 2020) remind us that:

> At its core, machine learning is the process of "showing" your statistical model only some of the data at once and training the model to predict accurately on that training dataset (this is the "learning" part of machine learning). Then, the model as developed on the training data is shown new data - data you had all along, but hid from your computer initially - and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data.

#### Training and Testing Sets

It is therefore common when beginning a modeling project to [separate the data set](https://bookdown.org/max/FES/data-splitting.html) into two partitions:

-   The *training set* is used to estimate, develop and compare models; feature engineering techniques; tune models, etc.

-   The *test set* is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.

There are different ways to create these partitions of the data and there is no uniform guideline for determining how much data should be set aside for testing. The proportion of data can be driven by many factors, including the size of the original pool of samples and the total number of predictors.

After you decide how much to set aside, the most common approach for actually partitioning your data is to use a random sample. For our purposes, we'll use random sampling to select 20% for the test set and use the remainder for the training set, which are the defaults for the {[rsample](https://tidymodels.github.io/rsample/)} package.

#### Split Data Sets

To split our data, we will be using our first {tidymodels} function - `initial_split()`.

The function `initial_split()` function from the {rsample} package takes the original data and saves the information on how to make the partitions. The {rsample} package has two aptly named functions for created a training and testing data set called `training()` and `testing()`, respectively.

We also specify the strata to ensure there is not misbalance in the dependent variable (good_grad_rate).

Run the following code to split the data:

```{r}
set.seed(20250712)

train_test_split <- initial_split(ipeds, prop = .80, strata = "good_grad_rate")

data_train <- training(train_test_split)

data_test  <- testing(train_test_split)
```

**Note**: Since random sampling uses random numbers, it is important to set the random number seed using the `set.seed()` function. This ensures that the random numbers can be reproduced at a later time (if needed). We pick the first date on which we may carry out this learning lab as the seed, but any number will work!

#### **ðŸ‘‰ Your Turn** **â¤µ**

Go ahead and type `data_train` and `data_test` into the console (in steps) to check that this data set indeed has 80% of the number of observations as in the larger data. Do that in the chunk below:

```{r}
data_train
data_test
```

### Step 2: Create a "Recipe"

In this section, we introduce another tidymodels package named {[recipes](https://recipes.tidymodels.org/)}, which is designed to help you prepare your data *before* training your model. Recipes are built as a series of preprocessing steps, such as:

-   converting qualitative predictors to indicator variables (also known as dummy variables),

-   transforming data to be on a different scale (e.g., taking the logarithm of a variable),

-   transforming whole groups of predictors together,

-   extracting key features from raw variables (e.g., getting the day of the week out of a date variable), and so on.

If you are familiar with R's formula interface, a lot of this might sound familiar and like what a formula already does. Recipes can be used to do many of the same things, but they have a much wider range of possibilities.

We'll dive more into the above steps in later modules, focusing on the formula for our model at this point. So, for now, we'll simply add the features we used in the last module, adding the new feature we created above.

#### **Add a formula**

To get started, let's create a recipe for a simple logistic regression model. Before training the model, we can use a recipe.

The [`recipe()`function](https://recipes.tidymodels.org/reference/recipe.html) as we used it here has two arguments:

-   A **formula**. Any variable on the left-hand side of the tilde (`~`) is considered the model outcome (`code`, in our present case). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (`.`) to indicate all other variables as predictors. This is where we can add functions to automate some of the feature engineering steps (in later modules).

-   The **data**. A recipe is associated with the data set used to create the model. This will typically be the *training* set, so `data = train_data` here. Naming a data set doesn't actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.

#### **ðŸ‘‰ Your Turn** **â¤µ**

Let's create a recipe where we predict `good_grad_rate` (the outcome variable) on the basis of several variables:

-   `tuition_fees`
-   `avg_salary`
-   `percent_fin_aid`
-   `tuition_to_salary_ratio` (the variable we created based on two of the above variables)

```{r}
my_rec <- recipe(good_grad_rate ~ 
                     tuition_fees +
                     avg_salary +
                     tuition_to_salary_ratio +
                     percent_fin_aid,
                 data = ipeds) 
```

### Step 3: Specify the model and workflow

With tidymodels, we start building a model by specifying the *functional form* of the model that we want using the [{parsnip} package](https://tidymodels.github.io/parsnip/). Since our outcome is binary, the model type we will use is "[logistic regression](https://parsnip.tidymodels.org/reference/logistic_reg.html)." We can declare this with `logistic_reg()` and assign to an object we will later use in our workflow:

Run the following code to finish specifying our model:

```{r}
# specify model
my_mod <-
    logistic_reg()
```

That is pretty underwhelming since, on its own, it doesn't really do much. However, now that the type of model has been specified, a method for *fitting* or training the model can be stated using the **engine**.

#### **Start your engine**

To set the *engine,* let's rewrite the code above and "pipe" in the `set_engine("glm")` function and `set_mode("classification"))` to set the "*mode*" to classification. Note that this could also be changed to regression for a continuous/numeric outcome.

#### **ðŸ‘‰ Your Turn** **â¤µ**

Below, specify a glm engine, and a classification mode, replacing the placeholder text below.

```{r}
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% # generalized linear model
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression

my_mod
```

The engine value is often a mash-up of different packages that can be used to fit or train the model as well as the estimation method. For example, we will use "glm" a generalized linear model for binary outcomes and default for logistic regression in the {parsnip} package.

#### **Add to workflow**

Now we can use the recipe created earlier across several steps as we train and test our model. To simplify this process, we can use a *model workflow*, which pairs a model and recipe together.

This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and testÂ *workflows*.

We'll use the{[workflows](https://workflows.tidymodels.org/)} package from tidymodels to bundle our parsnip model (`lr_mod`) with our first recipe (`lr_recipe_1`).

Add your model and recipe (see their names above)!

```{r}
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

### Step 4: Fit model

Now that we have a single workflow that can be used to prepare the recipe and train the model from the resulting predictors, we can use the `fit()` function to fit our model to our `train_data`. And again, we set a random number seed to ensure that if we run this same code again, we will get the same results in terms of the data partition:

Finally, we'll use the `last_fit` function, which is the key here: note that it uses the `train_test_split` data---not just the training data.

Here, then, we fit the data *using the training data set* and evaluate its accuracy using the *testing data set* (which is not used to train the model), passing the `my_wf` object as the first argument, and the split as the second.

```{r}
final_fit <- last_fit(my_wf, train_test_split)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Type the output of the above function (the name you assigned the output to) below; this is the final, fitted model---one that can be interpreted further in the next step!

```{r}
final_fit
```

You may see a message/warning above or when you examine `final_fit`; you can safely ignore that.

### Step 5: Interpret accuracy

Importantly, we can *summarize* across all of these codes. One way to do this is straightforward; how many of the codes were the same, as in the following chunk of code:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, good_grad_rate) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == good_grad_rate) # create a new variable, co
```

You may notice some of the rows may be missing values. This is because there were some missing values in the `imd_band` variable, and for this machine learning algorithm (the generalized linear model), missing values result in row-wise deletion.

When these are **the same**, the model predicted the code *correctly*; when they aren't the same, the model was incorrect.

We can also tabulate these using the `tabyl` function:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, good_grad_rate) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == good_grad_rate) %>%  # create a new variable, co
    tabyl(correct)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

A short-cut to the above is to simply use the `collect_metrics()` function, taking `final_fit` as its one argument; **we'll use this short-cut from here forward**, having seen how accuracy is calculated. Write that code below.

```{r}
collect_metrics(final_fit)
```

Let's step back a bit. How well *could* we do if we include more data? And how *useful* could such a model be in the real world? We'll dive into these questions more over the forthcoming modules.

That's it for now; the core parts of machine learning are used in the above steps you took; what we'll do after this leaning lab only adds nuance and complexity to what we've already done.

## 5. COMMUNICATE

For your SML Module 2 Badge, you will have an opportunity to create a simple "data product" designed to illustrate insights some insights gained from your model and ideally highlight an "action step" that can be taken to act upon your findings.

Rendered HTML files can be published online through a variety of ways including [Posit Cloud](https://posit.cloud/learn/guide#publish-from-cloud), [RPubs](#0) , [GitHub Pages](#0), [Quarto Pub](#0), or [other methods](#0). The easiest way to quickly publish your file online is to publish directly from RStudio. You can do so by clicking the "Publish" button located in the Viewer Pane after you render your document as illustrated in the screenshot below.

![](img/publish.png)

Congratulations - you've completed the second supervised machine learning case study!
