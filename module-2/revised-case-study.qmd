---
title: "Predicting Institutional Graduation Rates Using IPEDS Data"
subtitle: "Case Study"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
---

## 1. PREPARE

Conceptually, we focus on prediction and how it differs from the goals of description or explanation. We have two readings in Learning Lab 1 that accompany this. The first reading introduced below focuses on this distinction between prediction and description or explanation. It is one of the most widely-read papers in machine learning and articulates how machine learning differs from other kinds of statistical models. Breiman describes the difference in terms of *data modeling* (models for description and explanation) and *algorithmic modeling* (what we call prediction or machine learning models).

#### Research Question

Technically, we'll focus on the core parts of doing a machine learning analysis in R. We'll use the {[tidymodels](https://www.tidymodels.org/)} set of R packages (add-ons) to do so, like in the first module. However, to help anchor our analysis and provide us with some direction, we'll focus on the following research question as we explore this new approach:

> How well can we predict which institutions have higher graduation rates?

This builds directly on the work from Module 1, where we started investigating this question. In this module, we'll properly implement a training and testing workflow to evaluate our predictions more rigorously.

#### Reading: Statistical modeling: The two cultures

> Breiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). *Statistical Science, 16*(3), 199-231. <https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf>

**ðŸ‘‰ Your Turn** **â¤µ**

You'll be asked to reflect more deeply on this article later on (in the badge activity); but for now, open up the article and take a quick scan of the article and note below an observation or question you have about the article.

-   YOUR RESPONSE HERE

#### Reading: Predicting students' final grades

> Estrellado, R. A., Freer, E. A., Mostipak, J., Rosenberg, J. M., & VelÃ¡squez, I. C. (2020). *Data science in education using R*. Routledge (c14), Predicting students' final grades using machine learning methods with online course data. <http://www.datascienceineducation.com/>

Please review this chapter, focusing on the overall goals of the analysis and how the analysis was presented (focusing on predictions, rather than the ways we may typically interpret a statistical model--like measures of statistical significance).

### 1b. Load Packages

Like in the last module, please load the tidyverse package. Also, please load the tidymodels and janitor packages in the code chunk below.

```{r load-packages}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled (Krumm et al, 2018). In Part 2, we focus on the following wrangling processes to:

1.  **Importing and Inspecting Data**. In this section, we will "read" in our CSV data file and take a quick look at what our file contains.

2.  **Transform Variables**. We use the `mutate()` function to create a dichotomous variable for whether or not the institution has a "good" graduation rate, building on what we did in Module 1.

### 2a. Import and Inspect Data

For this module, we'll be working with the same IPEDS (Integrated Postsecondary Education Data System) dataset we used in Module 1. Let's read it in:

```{r}
ipeds <- read_csv("data/ipeds-all-title-9-2022-data.csv")
```

We'll use the same data cleaning steps from Module 1:

```{r}
ipeds <- janitor::clean_names(ipeds)
```

```{r}
ipeds <- ipeds %>% 
    select(name = institution_name, 
           title_iv = hd2022_postsecondary_and_title_iv_institution_indicator, # is the university a title IV university?
           carnegie_class = hd2022_carnegie_classification_2021_basic, # which carnegie classification
           state = hd2022_state_abbreviation, # state
           total_enroll = drvef2022_total_enrollment, # total enrollment
           pct_admitted = drvadm2022_percent_admitted_total, # percentage of applicants admitted
           n_bach = drvc2022_bachelors_degree, # number of students receiving a bachelor's degree
           n_mast = drvc2022_masters_degree, # number receiving a master's
           n_doc = drvc2022_doctors_degree_research_scholarship, # number receive a doctoral degree
           tuition_fees = drvic2022_tuition_and_fees_2021_22, # total cost of tuition and fees
           grad_rate = drvgr2022_graduation_rate_total_cohort, # graduation rate
           percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # percent of students receive financial aid
           avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks) # average salary of instructional staff
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Just as we did in Module 1, let's filter the data to include only Title IV postsecondary institutions.

```{r}
ipeds <- ipeds %>% 
    filter(title_iv == "Title IV postsecondary institution")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Now, filter the data again to *only* include institutions with a carnegie classification. Specifically, exclude those institutions with a value for the `carnegie_class` variable that is "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)". As a hint: whereas the logical operator `==` is used to include only matching conditions, the logical operator `!=` excludes matching conditions.

```{r}
ipeds <- ipeds %>% 
    filter(carnegie_class != "Not applicable, not in Carnegie universe (not accredited or nondegree-granting)")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Let's inspect our data - using `glimpse()` or another means of your choosing below.

```{r}
glimpse(ipeds)
```

Write down a few observations after inspecting the data:

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

### 2b. Transform Variables

We'll now transform our graduation rate variable into a binary outcome for our classification task, just as we did in Module 1.

#### **ðŸ‘‰ Your Turn** **â¤µ**

Create a binary variable called `good_grad_rate` that indicates whether an institution has a graduation rate above a certain threshold. Choose a threshold that seems reasonable to you.

```{r}
ipeds <- ipeds %>% 
    mutate(good_grad_rate = if_else(grad_rate > 62, 1, 0),
           good_grad_rate = as.factor(good_grad_rate))
```

Here, add a reason or two for how and why you picked the threshold you did:

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

## 3. EXPLORE

As noted by Krumm et al. (2018), exploratory data analysis often involves some combination of data visualization and *feature engineering*. In Part 3, we will create a quick visualization to help us spot any potential issues with our data and engineer new predictive variables or "features" that we will use in our predictive models.

### 3a. Examine Variables

Let's take a closer look at our institutional data. In the chunk below, count the number of institutions with good and poor graduation rates.

```{r}
ipeds %>% 
    count(good_grad_rate)
```

Next, let's visualize the distribution of graduation rates to better understand our data:

```{r}
ipeds %>% 
    ggplot(aes(x = grad_rate)) +
    geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
    labs(title = "Distribution of Graduation Rates",
         x = "Graduation Rate (%)",
         y = "Number of Institutions")
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Create another visualization that explores the relationship between two variables in our dataset. For example, you might want to look at the relationship between tuition fees and graduation rates, or enrollment and graduation rates.

```{r}
# Add your visualization code here
```

### 3b. Feature Engineering

As defined by Krumm, Means, and Bienkowski (2018) in *Learning Analytics Goes to School*:

> **Feature engineering** is the process of creating new variables within a dataset, which goes above and beyond the work of recoding and rescaling variables.

The authors note that feature engineering draws on substantive knowledge from theory or practice, experience with a particular data system, and general experience in data-intensive research. Moreover, these features can be used not only in machine learning models, but also in visualizations and tables comprising descriptive statistics.

Though not often discussed, feature engineering is an important element of data-intensive research that can generate new insights and improve predictive models. You can read more about feature engineering [here](https://www.tmwr.org/recipes.html).

For this module, we'll create a new feature that might help predict graduation rates. Let's create a derived variable representing the ratio of bachelor's degrees to total enrollment, which could indicate the institution's focus on undergraduate education completion.

```{r}
ipeds <- ipeds %>% 
    mutate(bach_completion_ratio = n_bach / total_enroll)
```

Let's examine this new feature:

```{r}
ipeds %>% 
    ggplot(aes(x = bach_completion_ratio, fill = good_grad_rate)) +
    geom_density(alpha = 0.5) +
    labs(title = "Distribution of Bachelor's Completion Ratio by Graduation Rate Category",
         x = "Bachelor's Degrees / Total Enrollment",
         y = "Density",
         fill = "Good Graduation Rate")
```

We're now ready to proceed to the five machine learning steps!

## 4. MODEL

In this step, we will dive into the SML modeling process in much more depth than in the last module.

1.  **Split Data** into a training and test set that will be used to develop a predictive model;

2.  **Create a "Recipe"** for our predictive model and learn how to deal with nominal data that we would like to use as predictors;

3.  **Specify the model and workflow** by selecting the *functional form* of the model that we want and using a *model workflow* to pair our model and recipe together;

4.  **Fit Models** to our training set using logistic regression;

5.  **Interpret Accuracy** of our model to see how well our model can "predict" our outcome of interest.

### Step 1. Split data

The authors of Data Science in Education Using R (Estrellado et al., 2020) remind us that:

> At its core, machine learning is the process of "showing" your statistical model only some of the data at once and training the model to predict accurately on that training dataset (this is the "learning" part of machine learning). Then, the model as developed on the training data is shown new data - data you had all along, but hid from your computer initially - and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data.

#### Training and Testing Sets

It is therefore common when beginning a modeling project to [separate the data set](https://bookdown.org/max/FES/data-splitting.html) into two partitions:

-   The *training set* is used to estimate, develop and compare models; feature engineering techniques; tune models, etc.

-   The *test set* is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.

There are different ways to create these partitions of the data and there is no uniform guideline for determining how much data should be set aside for testing. The proportion of data can be driven by many factors, including the size of the original pool of samples and the total number of predictors.

After you decide how much to set aside, the most common approach for actually partitioning your data is to use a random sample. For our purposes, we'll use random sampling to select 20% for the test set and use the remainder for the training set, which are the defaults for the {[rsample](https://tidymodels.github.io/rsample/)} package.

#### Split Data Sets

To split our data, we will be using our first {tidymodels} function - `initial_split()`.

The function `initial_split()` function from the {rsample} package takes the original data and saves the information on how to make the partitions. The {rsample} package has two aptly named functions for created a training and testing data set called `training()` and `testing()`, respectively.

We also specify the strata to ensure there is not misbalance in the dependent variable (good_grad_rate).

Run the following code to split the data:

```{r}
set.seed(20230712)

train_test_split <- initial_split(ipeds, prop = .80, strata = "good_grad_rate")

data_train <- training(train_test_split)

data_test  <- testing(train_test_split)
```

**Note**: Since random sampling uses random numbers, it is important to set the random number seed using the `set.seed()` function. This ensures that the random numbers can be reproduced at a later time (if needed). We pick the first date on which we may carry out this learning lab as the seed, but any number will work!

#### **ðŸ‘‰ Your Turn** **â¤µ**

Go ahead and type `data_train` and `data_test` into the console (in steps) to check that this data set indeed has 80% of the number of observations as in the larger data. Do that in the chunk below:

```{r}
data_train
data_test
```

### Step 2: Create a "Recipe"

In this section, we introduce another tidymodels package named {[recipes](https://recipes.tidymodels.org/)}, which is designed to help you prepare your data *before* training your model.
