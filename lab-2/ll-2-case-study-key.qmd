---
title: "Who Drops Online Courses?"
subtitle: "ML Module 2: Case Study Key"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
# bibliography: lit/references.bib
resource_files:
- img/tidymodels.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

## 1. PREPARE

Each machine learning "case study" is designed to illustrate how machine
learning methods and techniques can be applied to address a research
question of interest, create useful data products, and conduct
reproducible research. Each case study is structured around a basic
analytics workflow modeled after the Data-Intensive Research Workflow
from [Learning Analytics Goes to School](#0)Â (Krumm et al., 2018):

![](https://sbkellogg.github.io/eci-589/unit-1/img/workflow.png){alt=""
width="80%"}

Figure 2.2 Steps of Data-Intensive Research Workflow

In the overview presentation for this learning module, we considered
five steps in our supervised machine learning process. Those steps are
mirrored here in this case study, with the addition of some other
components of this workflow. For example, to help prepare for analysis,
we'll first take a step back and think about how we want to use machine
learning, and *predicting* is a key word. Many scholars have focused on
predicting students who are *at-risk*: of dropping a course or not
succeeding in it. In the ML Learning module 1 case study will cover the
following workflow topics as we attempt to develop our own model for
predicting student drop-out:

1.  **Prepare**: Prior to analysis, we'll look at the context from which
    our data came, formulate a basic research question, and get
    introduced the {tidymodels} packages for machine learning.

2.  **Wrangle**: Wrangling data entails the work of cleaning,
    transforming, and merging data. In Part 2 we focus on importing CSV
    files and modifying some of our variables.

3.  **Explore**: We take a quick look at our variables of interest and
    do some basic "feature engineering" by creating some new variables
    we think will be predictive of students at risk.

4.  **Model:** We dive deeper into the five steps in our supervised
    machine learning process, focusing on the mechanics of **making
    predictions**.

5.  **Communicate:** To wrap up our case study, we'll create our first
    "data product" and share our analyses and findings by creating our
    first web page using R Markdown.

### 1a. Conceptual Focus

Conceptually, we focus on prediction and how it differs from the goals
of description or explanation. We have two readings in Learning module 1
that accompany this. The first reading introduced below focuses on this
distinction between prediction and description or explanation. It is one
of the most widely-read papers in machine learning and articulates how
machine learning differs from other kinds of statistical models. Breiman
describes the difference in terms of *data modeling* (models for
description and explanation) and *algorithmic modeling* (what we call
prediction or machine learning models)\*

#### Research Question

Technically, we'll focus on the core parts of doing a machine learning
analysis in R. We'll use the {[tidymodels](https://www.tidymodels.org/)}
set of R packages (add-ons) to do so. However, to help anchor our
analysis and provide us with some direction, we'll focus on the
following research question as we explore this new:

> How well can we predict students who are at risk of dropping a course?

#### Reading 1: Statistical modeling: The two cultures

![](img/breiman.png){width="50%"}

Breiman, L. (2001). Statistical modeling: The two cultures (with
comments and a rejoinder by the author). *Statistical Science, 16*(3),
199-231.
<https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf>

**Abstract**

There are two cultures in the use of statistical modeling to reach
conclusions from data. One assumes that the data are generated by a
given stochastic data model. The other uses algorithmic models and
treats the data mechanism as unknown. The statistical community has been
committed to the almost exclusive use of data models. This commitment
has led to irrelevant theory, questionable conclusions, and has kept
statisticians from working on a large range of interesting current
problems. Algorithmic modeling, both in theory and practice, has
developed rapidly in fields outside statistics. It can be used both on
large complex data sets and as a more accurate and informative
alternative to data modeling on smaller data sets. If our goal as a
field is to use data to solve problems, then we need to move away from
exclusive dependence on data models and adopt a more diverse set of
tools.

#### **ðŸ‘‰ Your Turn** **â¤µ**

You'll be asked to reflect more deeply on this article later on (in the
badge activity); but for now, open up the article and take quick scan of
the article and note below an observation or question you have about the
article.

-   YOUR RESPONSE HERE

### 1b. Load Packages

As highlighted inÂ [Chapter 6 of Data Science in Education Using
R](https://datascienceineducation.com/c06.html)Â (DSIEUR), one of the
first steps of every workflow should be to set up your "Project" within
RStudio. Recall that:

> A **Project** is the home for all of the files, images, reports, and
> code that are used in any given project

Since we are working from an R project cloned from
[GitHub](https://github.com/laser-institute), a Project has already been
set up for you as indicated by the `.Rproj` file in your main directory
in the Files pane. Instead, we will focus on getting our project set up
withe the requisite packages we'll need for analysis.

**Packages**, sometimes called libraries, are shareable collections of R
code that can contain functions, data, and/or documentation and extend
the functionality of R. You can always check to see which packages have
already been installed and loaded into RStudio Cloud by looking at the
the Files, Plots, & Packages Pane in the lower right-hand corner.

Two packages we'll use extensively throughout these learning labs are
the {tidyverse} and {tidymodels} packages.

#### tidyverse ðŸ“¦

![](img/tidyverse.png){width="20%"}

One package that we'll be using extensively throughout LASER is the
{tidyverse} package. Recall from earlier tutorials that the {tidyverse}
package is actually a [collection of R
packages](https://www.tidyverse.org/packages) designed for reading,
wrangling, and exploring data and which all share an underlying design
philosophy, grammar, and data structures. These shared features are
sometimes "tidy data principles."

Click the green arrow in the right corner of the "code chunk" that
follows to load the {tidyverse} library.

```{r load-tidyverse}

library(tidyverse)

```

#### **tidymodels ðŸ“¦**

![](img/tidymodels.png){width="20%"}

The [tidymodels](https://www.tidymodels.org/) package is a
"meta-package" for modeling and statistical analysis that shares the
underlying design philosophy, grammar, and data structures of the
[tidyverse](https://www.tidyverse.org/). It includes a core set of
packages that are loaded on startup and contains tools for:

-   data splitting and pre-processing;

-   model selection, tuning, and evaluation;

-   feature selection and variable importance estimation;

-   as well as other functionality.

#### **ðŸ‘‰ Your Turn** **â¤µ**

In addition to the {tidymodels} package, we'll also be using the
lightweight but highly useful {janitor} package to help with some data
cleaning tasks. Use the code chunk below to load these two packages:

```{r load-tidymodels}

library(tidymodels)
library(janitor)

```

As a tip, remember to use the `library()` function to load these
packages. After you've done that, click the green arrow to run the code
chunk. If you see a bunch of messages (not anything labeled as an
error), you are good to go! These messages mean the packages loaded
correctly.

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data (Wickham & Grolemund, 2017).
The importance of data wrangling is difficult to overstate, as it
involves the initial steps of going from raw data to a dataset that can
be explored and modeled (Krumm et al, 2018). In Part 2, we focus on the
the following wrangling processes to:

1.  **Importing and Inspecting Data**. In this section, we will "read"
    in our CSV data file and take a quick look at what our file
    contains.

2.  **Mutate Variables**. We use the `mutate()` function to create a
    dichotomous variable for whether or not the student withdrew from
    the course.

### 1a. Import and Inspect Data

For learning labs 1-3, we'll be using a widely-used data set in the
learning analytics field: the [Open University Learning Analytics
Dataset (OULAD)](https://analyse.kmi.open.ac.uk/open_dataset). The OULAD
was created by learning analytics researchers at the United
Kingdom-based Open University. It includes data from post-secondary
learners participation in one of several Massive Open Online Courses
(called *modules* in the OULAD).

> Kuzilek, J., Hlosta, M., & Zdrahal, Z. (2017). Open university
> learning analytics dataset. *Scientific Data, 4(*1), 1-8.
> <https://www.nature.com/articles/sdata2017171>

**Abstract**

Learning Analytics focuses on the collection and analysis of learners'
data to improve their learning experience by providing informed guidance
and to optimise learning materials. To support the research in this area
we have developed a dataset, containing data from courses presented at
the Open University (OU). What makes the dataset unique is the fact that
it contains demographic data together with aggregated clickstream data
of students' interactions in the Virtual Learning Environment (VLE).
This enables the analysis of student behavior, represented by their
actions. The dataset contains the information about 22 courses, 32,593
students, their assessment results, and logs of their interactions with
the VLE represented by daily summaries of student clicks (10,655,280
entries). The dataset is freely available at
<https://analyse.kmi.open.ac.uk/open_dataset> under a CC-BY 4.0 license.

#### **ðŸ‘‰ Your Turn** **â¤µ**

You don't need to read the entire article yet, but please open this
article, scan the sections, and write down two things you notice or
wonder about the dataset.

1.  YOUR RESPONSE HERE

2.  YOUR RESPONSE HERE

#### Read CSV Data File

The data can be downloaded at the above link; however, for our purposes,
they are already downloaded to the `data` sub-folder.

We'll use the `read_csv()` function to load the files -- two in total,
with data on students and assessments. Note: we have done some minimal
processing of these files to make getting us started easier. If you're
interested in what we've done, check out the `oulad.R` file in the
`lab-1` folder.

For now, please read in the `oulad-students.csv` file. Use the
`read_csv()` function to do this, paying attention to where those files
are located relative to this case study file.

```{r}
students <- read_csv("data/oulad-students.csv")
```

You can see a description of the data
[here](https://analyse.kmi.open.ac.uk/open_dataset#description). The
students file includes three files joined together: studentInfo,
courses, and studentRegistration. Take a look at the data description to
get a sense for what variables are in which data frame.

#### **ðŸ‘‰ Your Turn** **â¤µ**

#### Inspect Data

In the chunk below, examine the data set using a function or means of
your choice (such as just *printing* the data set by typing its name or
using the `glimpse()` function). Do this in the code chunk below! Note
its dimensions --- especially how many rows it has!

```{r}
glimpse(students)
```

Write down a few observations after inspecting the data:

-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE
-   YOUR RESPONSE HERE

### 2b. "Mutate" Variables

We're going to do a few more steps related to data wrangling here,
noting we could also do these at later stages of our process (namely, in
the feature engineering stage).

First, since we are interested in developing a model that can predict
whether a student is at risk of dropping a course, and so we can
intervene before that happens, we need an outcome variable that let's us
know if they have passed.

To create this variable, let's use the `mutate()` function to create a
dichotomous variable for whether or not the student withdrew from the
course. Here's a way we can do this, using `if_else()` and
`as.factor()`. This will be our *outcome* variable, or the predicted
variable.

```{r}
students <- students %>% 
    mutate(pass = ifelse(final_result == "Pass", 1, 0)) %>% # creates a new variable named "pass" and a dummy code of 1 if value of final_result equals "pass" and 0 if not
    mutate(pass = as.factor(pass)) # makes the variable a factor, helping later steps
```

**Note**: The [`mutate()`
function](https://dplyr.tidyverse.org/reference/mutate.html#:~:text=mutate()%20creates%20new%20columns,setting%20their%20value%20to%20NULL%20).)
is a critical function to learn and is used to create new columns that
are functions of existing variables. It can also modify (if the name is
the same as an existing column) and delete columns (by setting their
value to `NULL`).

Next, let's do something similar for whether a student identifies as
having a disability. In this case, there are only two values for the
disability variable, so we can simply convert it directly to a factor.
Look at the code you used above, *modifying* it for the disability
variable. This will be an *independent variable*, or a predictor
variable.

```{r}
students <- students %>% 
    mutate(disability = as.factor(disability))
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

In the chunk below, use the `view()` function to manually check and see
if our new variable has indeed been added to our data frame.

```{r, eval=FALSE}
view(students)
```

## 3. EXPLORE

As noted by Krumm et al.Â (2018), exploratory data analysis often
involves some combination of data visualization and *feature
engineering*. In Part 3, we will create a quick visualization to help us
spot any potential issues with our data and engineer new predictive
variables or "features" that we will use in our predictive models.
Specifically, in Part 3 we will:

1.  **Examine Outcomes** by taking a quick `count()` of the number of
    students and the number of specific offerings of each course module.

2.  **Engineer Predictors** by creating one more predictor variable
    based on a measure of socioeconomic resources--the index of multiple
    depravity variable.

### 3a. Examine Variables

A useful function for exploring data is `count()`; it does what it
sounds like! It counts how many times values for a variable appear.

Referring to the [data
description](https://analyse.kmi.open.ac.uk/open_dataset#description),
in the chunk below, count the number of students. Also, count the number
of courses (modules) and specific offerings (as modules can be offered
multiple times per year). Learn more about `count()`
[here](https://dplyr.tidyverse.org/reference/count.html).

```{r}
students %>% 
    count(id_student) # this many students

students %>% 
    count(code_module, code_presentation) # this many offerings
```

### 3b. Feature Engineering

As defined by Krumm, Means, and Bienkowski (2018) in *Learning Analytics
Goes to School*:

> **Feature engineering** is the process of creating new variables
> within a dataset, which goes above and beyond the work of recoding and
> rescaling variables.

The authors note that feature engineering draws on substantive knowledge
from theory or practice, experience with a particular data system, and
general experience in data-intensive research. Moreover, these features
can be used not only in machine learning models, but also in
visualizations and tables comprising descriptive statistics.

Though not often discussed, feature engineering is an important element
of data-intensive research that can generate new insights and improve
predictive models. You can read more about feature engineering
[here](https://www.tmwr.org/recipes.html).

#### Student Socioeconomic Index

For our first module, we'll engage in a very basic feature engineering
step, though we'll do this *much* more in the next learning lab.

To do feature engineering, let's create one more predictor variable
based on a measure of socioeconomic resources--the index of multiple
depravity variable. The process we take here is to turn this variable
that is a character string into a number by creating a factor and then
*coercing* it to an integer.

#### **ðŸ‘‰ Your Turn** **â¤µ**

Please replace the \_\_\_ values in the code below with the correct
variable.

```{r}
students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels

students
```

We're now ready to proceed to the five machine learning steps!

## 4. MODEL

Recall from our first reading that there are two general types of
modeling approaches: unsupervised and supervised machine learning. In
Part 4, we focus on supervised learning models, which are used to
quantify relationships between features (e.g., motivation and
performance) and a known outcome (e.g., student drop out). These models
can be used for classification of binary or categorical outcomes, as
we'll illustrate in this section, or regression as we'll demonstrate in
Learning module 2.

Specifically, in Part 4 we will learn how to:

1.  **Split Data** into a training and test set that will be used to
    develop a predictive model;

2.  **Create a "Recipe"** for our predictive model and learn how to deal
    with nominal data that we would like to use as predictors;

3.  **Specify the model and workflow** by selecting the *functional
    form* of the model that we want and using a *model workflow* to pair
    our model and recipe together;

4.  **Fit Models** to our training set using logistic regression;

5.  **Interpret Accuracy** of our model to see how well our model can
    "predict" our outcome of interest.

### Step 1. Split data

The authors of Data Science in Education Using R (Estrellado et
al.,2020) remind us that:

> At its core, machine learning is the process of "showing" your
> statistical model only some of the data at once and training the model
> to predict accurately on that training dataset (this is the "learning"
> part of machine learning). Then, the model as developed on the
> training data is shown new data - data you had all along, but hid from
> your computer initially - and you see how well the model that you
> developed on the training data performs on this new testing data.
> Eventually, you might use the model on entirely new data.

#### Training and Testing Sets

It is therefore common when beginning a modeling project to [separate
the data set](https://bookdown.org/max/FES/data-splitting.html) into two
partitions:

-   The *training set* is used to estimate, develop and compare models;
    feature engineering techniques; tune models, etc.

-   The *test set* is held in reserve until the end of the project, at
    which point there should only be one or two models under serious
    consideration. It is used as an unbiased source for measuring final
    model performance.

There are different ways to create these partitions of the data and
there is no uniform guideline for determining how much data should be
set aside for testing. The proportion of data can be driven by many
factors, including the size of the original pool of samples and the
total number of predictors.Â 

After you decide how much to set aside, the most common approach for
actually partitioning your data is to use a random sample. For our
purposes, we'll use random sampling to select 20% for the test set and
use the remainder for the training set, which are the defaults for the
{[rsample](https://tidymodels.github.io/rsample/)} package.

#### Split Data Sets

To split our data, we will be using our first {tidymodels} function -
`initial_split()`.

The function `initial_split()` function from the {rsample} package takes
the original data and saves the information on how to make the
partitions. The {rsample} package has two aptly named functions for
created a training and testing data set called `training()` and
`testing()`, respectively.

Run the following code to split the data:

```{r}
set.seed(20230712)

train_test_split <- initial_split(students, prop = .80)

data_train <- training(train_test_split)

data_test  <- testing(train_test_split)
```

**Note**: Since random sampling uses random numbers, it is important to
set the random number seed using the `set.seed()` function. This ensures
that the random numbers can be reproduced at a later time (if needed).
We pick the first date on which we may carry out this learning module as
the seed, but any number will work!

#### **ðŸ‘‰ Your Turn** **â¤µ**

Go ahead and type `data_train` and `data_test` into the console (in
steps) to check that this data set indeed has 80% of the number of
observations as in the larger data. Do that in the chunk below:

```{r}
data_train

data_test
```

### Step 2: Create a "Recipe"

In this section, we introduce another tidymodels package named
{[recipes](https://recipes.tidymodels.org/)}, which is designed to help
you prepare your data *before* training your model. Recipes are built as
a series of preprocessing steps, such as:

-   converting qualitative predictors to indicator variables (also known
    as dummy variables),

-   transforming data to be on a different scale (e.g., taking the
    logarithm of a variable),

-   transforming whole groups of predictors together,

-   extracting key features from raw variables (e.g., getting the day of
    the week out of a date variable), and so on.

If you are familiar with R's formula interface, a lot of this might
sound familiar and like what a formula already does. Recipes can be used
to do many of the same things, but they have a much wider range of
possibilities.

#### **Add a formula**

To get started, let's create a recipe for a simple logistic regression
model. Before training the model, we can use a recipe.

The
[`recipe()`function](https://recipes.tidymodels.org/reference/recipe.html)
as we used it here has two arguments:

-   A **formula**. Any variable on the left-hand side of the tilde (`~`)
    is considered the model outcome (`code`, in our present case). On
    the right-hand side of the tilde are the predictors. Variables may
    be listed by name, or you can use the dot (`.`) to indicate all
    other variables as predictors.

-   The **data**. A recipe is associated with the data set used to
    create the model. This will typically be the *training* set, so
    `data = train_data` here. Naming a data set doesn't actually change
    the data itself; it is only used to catalog the names of the
    variables and their types, like factors, integers, dates, etc.

Let's create a recipe where we predict `pass` (the outcome variable) on
the basis of the `disability` and `imd_band` (predictor) variables.

```{r}
my_rec <- recipe(pass ~ disability + imd_band, data = data_train)

my_rec
```

### Step 3: Specify the model and workflow

With tidymodels, we start building a model by specifying the *functional
form* of the model that we want using the [{parsnip}
package](https://tidymodels.github.io/parsnip/). Since our outcome is
binary, the model type we will use isÂ "[logistic
regression](https://parsnip.tidymodels.org/reference/logistic_reg.html)."
We can declare this with `logistic_reg()` and assign to an object we
will later use in our workflow:

Run the following code to finish specifying our model:

```{r}
# specify model
my_mod <-
    logistic_reg()
```

That is pretty underwhelming since, on its own, it doesn't really do
much. However, now that the type of model has been specified, a method
forÂ *fitting*Â or training the model can be stated using the **engine**.

#### **Start your engine**

To set the *engine,* let's rewrite the code above and "pipe" in the
`set_engine("glm")` function and `set_mode("classification"))` to set
the "*mode*" to classification. Note that this could also be changed to
regression for a continuous/numeric outcome.

Run the following code to finish specifying our model:

```{r}
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% # generalized linear model
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression

my_mod
```

The engine value is often a mash-up of different packages that can be
used to fit or train the model as well as the estimation method. For
example, we will use "glm" a generalized linear model for binary
outcomes and default for logistic regression in the {parsnip} package.

#### **Add to workflow**

Now we can use the recipe created earlier across several steps as we
train and test our model. To simplify this process, we can use a *model
workflow*, which pairs a model and recipe together.

This is a straightforward approach because different recipes are often
needed for different models, so when a model and recipe are bundled, it
becomes easier to train and testÂ *workflows*.

We'll use the{[workflows](https://workflows.tidymodels.org/)} package
from tidymodels to bundle our parsnip model (`lr_mod`) with our first
recipe (`lr_recipe_1`).

```{r}
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

### Step 4: Fit model

Now that we have a single workflow that can be used to prepare the
recipe and train the model from the resulting predictors, we can use the
`fit()` function to fit our model to our `train_data`. And again, we set
a random number seed to ensure that if we run this same code again, we
will get the same results in terms of the data partition:

Finally, we'll fit our model.

```{r}
fitted_model <- fit(my_wf, data = data_train)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Importantly, here, we can look at the model. Type `fitted_model` in a
code chunk to take a look.

```{r}
fitted_model
```

Note that while we don't typically interpret the coefficients for a
machine learning model, it's important to recognize that many models
*do* produce coefficients we *could* interpret. Instead, we focus on how
the model does with respect to predicting the dependent variable.

**Observations**

Write down a brief observation of the output above, focusing on the
coefficients:

-   YOUR RESPONSE HERE

#### The last_fit function

Finally, we'll use the `last_fit` function, which is the key here: note
that it uses the `train_test_split` data---not just the training data.

Here, then, we fit the data *using the training data set* and evaluate
its accuracy using the *testing data set* (which is not used to train
the model).

```{r}
final_fit <- last_fit(my_wf, train_test_split)
```

Type `final_fit` below; this is the final, fitted model---one that can
be interpreted further in the next step!

```{r}
final_fit
```

You may see a message/warning above or when you examine `final_fit`; you
can safely ignore that.

### Step 5: Interpret accuracy

Run the code below to examine the predictions for the *test* split of
data. Note that the row ID is in the output below, but this doesn't
correspond one-one to the ID variables used in the presentation/Shiny.

```{r}
# collect test split predictions
final_fit %>%
    collect_predictions()
```

This is our first set of real output! Note two things:

1.  `.pres_class`: This is the *predicted* code
2.  `pass`: This is the known *code*

When these are **the same**, the model predicted the code *correctly*;
when they aren't the same, the model was incorrect.

Importantly, we can *summarize* across all of these codes. One way to do
this is straightforward; how many of the codes were the same, as in the
following chunk of code:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, pass) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == pass) # create a new variable, correct, telling us when the model was and was not correct
```

You may notice some of the rows may be missing values. This is because
there were some missing values in the `imd_band` variable, and for this
machine learning algorithm (the generalized linear model), missing
values result in row-wise deletion.

That's helpful, but there's one more step we can take -- counting up the
values of `correct`:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, pass) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == pass) %>% # create a new variable, correct, telling us when the model was and was not correct
    tabyl(correct)
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Let's interpret the above. If the value of `correct` is `TRUE` when the
predicted and known code are the same, what does the `percent` column
tell us? Add one or more notes to the dashes below:

-   YOUR RESPONSE HERE

A short-cut to the above is below; **we'll use this short-cut from here
forward**, having seen how accuracy is calculated.

```{r}
final_fit %>% 
    collect_metrics()
```

How accurate was our predictive model? Consider how well our model would
have done by chance alone -- what would the accuracy be in that case
(with the model predicting pass one-half of the time)?

```{r}
students %>% 
    count(pass)

students %>% 
    mutate(prediction = sample(c(0, 1), nrow(students), replace = TRUE)) %>% 
    mutate(correct = if_else(prediction == 1 & pass == 1 |
               prediction == 0 & pass == 0, 1, 0)) %>% 
    tabyl(correct)
```

Curiously, randomly picking a 0 (did not withdraw) or a 1 (pass) will
*always* lead to around a 50% accuracy, regardless of how many
observations are actually associated with a 0 or a 1.

**Observation**

Let's step back a bit. How well *could* we do if we include more data?
And how *useful* could such a model be in the real world? We'll dive
into these questions more over the forthcoming learning labs.

That's it for now; the core parts of machine learning are used in the
above steps you took; what we'll do after this leaning module only adds
nuance and complexity to what we've already done.

## 5. COMMUNICATE

The final step in the workflow/process is sharing the results of your
analysis with wider audience. Krumm et al. (2018) have outlined the
following 3-step process for communicating with education stakeholders
findings from an analysis:

1.  **Select.** Communicating what one has learned involves selecting
    among those analyses that are most important and most useful to an
    intended audience, as well as selecting a form for displaying that
    information, such as a graph or table in static or interactive form,
    i.e.Â a "data product."

2.  **Polish**. After creating initial versions of data products,
    research teams often spend time refining or polishing them, by
    adding or editing titles, labels, and notations and by working with
    colors and shapes to highlight key points.

3.  **Narrate.** Writing a narrative to accompany the data products
    involves, at a minimum, pairing a data product with its related
    research question, describing how best to interpret the data
    product, and explaining the ways in which the data product helps
    answer the research question.

For your learning ML Learning module 1 Badge, you will have an
opportunity to create a simple "data product" designed to illustrate
insights some insights gained from your model and ideally highlight an
"action step" that can be taken to act upon your findings.

#### **ðŸ‘‰ Your Turn** **â¤µ**

For now, we will wrap up this case study by converting our work into a
webpage that can be used to communicate your learning and demonstrate
some of your new R skills. To do so, you will need to "knit" your
document by clicking the
![](img/Screenshot%202023-06-28%20at%204.14.22%20PM.png){width="10%"}
button in the menu bar at the the top of this file. This will do two
things; it will:

1.  check through all your code for any errors; and,

2.  create a file in your directory that you can use to share you work
    through [Posit
    Cloud](https://posit.cloud/learn/guide#publish-from-cloud) (see
    screenshot example below to publish), [RPubs](#0) , [GitHub
    Pages](#0), [Quarto Pub](#0), or any [other methods](#0).

**Note:** Before knitting, make sure you change the `author:` name in
the YAML header at the top of this document so you can take credit for
your hard work!

1.  

![](img/knit-publish.png){width="80%"}

Congratulations - you've completed the first machine learning case
study!
