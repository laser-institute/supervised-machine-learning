---
title: "Machine Learning Learning Lab 3: Model Tuning"
subtitle: "Overview Presentation"
author: "**Dr. Joshua Rosenberg**"
institute: "LASER Institute"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  xaringan::moon_reader:
    css:
     - default
     - css/laser.css
     - css/laser-fonts.css
    lib_dir: libs                        # creates directory for libraries
    seal: false                          # false: custom title slide
    nature:
      highlightStyle: default         # highlighting syntax for code
      highlightLines: true               # true: enables code line highlighting 
      highlightLanguage: ["r"]           # languages to highlight
      countIncrementalSlides: false      # false: disables counting of incremental slides
      ratio: "16:9"                      # 4:3 for standard size,16:9
      slideNumberFormat: |
       <div class="progress-bar-container">
        <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
        </div>
       </div>
---
class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```
```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`

---

# Background

- Once we've processed our variables in new ways and have made our model better, we want to select the _best possible model_
- In this learning lab, we return to the #NGSSchat data set to see how much better we can make our predictive model
- Specifically, we'll build on our use of a relatively simple model - a logistic regression - by specifying a more complex model, a random forest, though the points and process apply to any complex algorithm (i.e., support vector machine, neural network)

---

# Agenda

.pull-left[

## Part 1: Core Concepts
### Model tuning
- hyperparameters
- random fores

]

.pull-right[

## Part 2: R Code-Along
### NGSSchat
- NGSS and transactional and substantive (again)
- Using a grid to estimate tuning parameters

]

---

class: clear, inverse, center, middle

# Core Concepts

---

# Discussion

.pull-left[
### Turn to an elbow partner and discuss:

- What do you think the benefits of fitting a more complex model may be?
- Which more complex models - if any - are you familiar with?
- What "costs" or downsides - if any - might accompany the estimation of a more complex model?

]

.pull-right[

```{r, echo = FALSE, out.width = "75%"}
knitr::include_graphics("img/joro-pointing.jpeg")
```

]
---

# How do I select a "final" or "best" model?

One general principle is to **start with the simplest useful model** and to _build toward
more complex models as helpful_.

This principle applies in multiple ways:

- To choose an algorithm, start with simpler models that you can efficiently use and understand
- To carry out feature engineering, understand your predictors well by starting with a subset
- To tune an algorithm, start with a relatively simple set of tuning parameters

This isn't just for beginners or those of us in education; [most spam filters use Support Vector Machines (and used Naive Bayes until recently)](https://vas3k.com/blog/machine_learning/) due to their combination of effectiveness and efficiency "in production."

---

# The bias-variance tradeoff

- The above consideration also matters in the context of the bias-variance trade-off.

- An important way to achieve good performance with test data is to balance between the inherent _bias_ in your algorithm and the _variance_ in the predictions of your algorithm; this is referred to as the **bias-variance** trade-off of _all_ models

---

# Illustrating the bias-variance tradeoff

```{r, echo = FALSE, fig.align="center", message = FALSE}
library(ggplot2)

set.seed(5)
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  ylab("pred")

ggsave("img/bias-variance-data-1.png")
```

---

# Strong bias

```{r, echo = FALSE, fig.align="center", message = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = "lm") +
  ylab("pred")

ggsave("img/bias-variance-data-3.png")
```

---

# A much less-biased algorithm

```{r, echo = FALSE, fig.align="center", message = FALSE}
set.seed(5)
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 1.3) * 15 + 3 * (x - 4)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) +
  ylab("pred")

ggsave("img/bias-variance-data-5.png")
```

---

# Slightly different data (right pane)

```{r, echo = FALSE, echo = FALSE, message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  ylab("pred")

ggsave("img/bias-variance-data-2.png")
```

.pull-left[
<img src="img/bias-variance-data-1.png" width=400 height=400>
]

.pull-right[
<img src="img/bias-variance-data-2.png" width=400 height=400>
]

---

# Still strong bias, but low variance

```{r, echo = FALSE, fig.align="center", message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = "lm") +
  ylab("pred")

ggsave("img/bias-variance-data-4.png")
```

.pull-left[
<img src="img/bias-variance-data-3.png" width=400 height=400>
]

.pull-right[
<img src="img/bias-variance-data-4.png" width=400 height=400>
]

---

# Low bias, but very high variance

```{r, echo = FALSE, fig.align="center", message = FALSE, include = FALSE}
dat = data.frame(x = runif(1000, 0, 12))
dat$x = sort(dat$x)
dat$y = with(dat, sin(x * 2.3) * 25 + 3 * (x - 3)^2)
sigma = with(dat, (exp(x - 5)/(1 + exp(x - 5)) - exp(x - 7)/(1 + exp(x - 7)) * 
    2) + 1.4) * 6
dat$yobs = dat$y + rnorm(nrow(dat), mean = 0, sd = sigma)

ggplot(dat, aes(x = x, y = yobs)) +
  geom_point(color = "darkgray") +
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) +
  ylab("pred")

ggsave("img/bias-variance-data-6.png")
```

.pull-left[
<img src="img/bias-variance-data-5.png" width=400 height=400>
]

.pull-right[
<img src="img/bias-variance-data-6.png" width=400 height=400>
]

---

# The bias-variance tradeoff

.pull-left[

#### Bias

- *Definition*: Difference between our known codes/outcomes and our predicted codes/outcomes; difference between $y$ and $\hat{y}$

- How (in)correct our models' (algorithms') predictions are

- Models with high bias can fail to capture important relationships--they can be *under-fit* to our data

- In short, how well our model reflects the patterns in the data

]

.pull-right[

#### Variance

- *Definition*: Using a different sample of data, the difference in $\hat{y}$ values

- How sensitive our predictions are to the specific sample on which we trained the model 
- Models with high variance can fail to predict different data well--they can be *over-fit* to our data

- In short, how stable the predictions of our model are

<h4><center>Regardless of model, we often wish to balance between bias and variance</center></h4>

---

# Tuning

- Many parts of models - their _parameters_ - are estimated from the data
- Other parts cannot be estimated from the data and must be used:
    - They are often set as defaults
    - But you can often improve on these defaults
- These are _hyperparameters_ 

---

# Aside: Random forests and classification trees

- *Random forests* are extensions of classification trees
- _Classification trees_ are a type of algorithm that - at their core - use conditional logic ("if-then" statements) in a _nested_ manner
    - For instance, here's a _very, very_ simple tree (from [APM](https://link.springer.com/book/10.1007/978-1-4614-6849-3)):

```{code}
if Predictor B >= 0.197 then
| if Predictor A >= 0.13 then Class = 1
| else Class = 2
else Class = 2
```

- Measures are used to determine the splits in such a way that classifies observations into small, homogeneous groups (using measures such as the Gini index and entropy measure)

---

# A more complex tree

```{code}
if Predictor B >= 0.197 then
| if Predictor A >= 0.13 then
    | if Predictor C < -1.04 then Class = 1
    | else Class = 2
else Class = 3
```

As you can imagine, with many variables, these trees can become very complex

---

# Random forests

- Random forest is an extension of decision tree modeling, whereby a collection of decision trees are simultaneously estimated ("grown") and are evaluated based on out-of-sample predictive accuracy
- Random forest estimates all the decision trees at once so each tree is independent of every other tree. 
    - The random forest algorithm provides a regression approach that is distinct from other modeling approaches. 
    - The final random forest model aggregates the findings across all the separate trees in the forest

---

```{r, echo = FALSE}
knitr::include_graphics("https://miro.medium.com/max/1184/1*i0o8mjFfCn-uD79-F1Cqkw.png")
```

[Koehrsen (2017)](https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d)

---

# Tuning parameters for random forests

- There are several important tuning parameters for these models:
    - the number of predictor variables that are randomly sampled for each split in the tree (`mtry` in the code we'll run later)
    - the number of data points required to execute a split (`min_n`)
    - the minimum number of observations in each node (`size`)
- These tuning parameters, broadly, balance predictive performance with the training data with how well the model will perform on new data 

---

# Overview of classification modeling

1. **Split data** (Prepare)  
1. **Engineer features and write down the recipe** (Wrangle and Explore)  
1. **Specify the model and workflow** (Model)  
1. **Fit model** (Model)
1. **Evaluate accuracy** (Communicate)  

---

class: clear, inverse, center, middle

# Code Examples

---

.panelset[

.panel[.panel-name[0]

**Prepare**

```{r panel-chunk-0, echo = TRUE, eval = FALSE}
library(tidyverse)
library(tidymodels)
library(here)
library(vip) # a new package we're adding for variable importance measures

d <- read_csv(here("data", "ngsschat-processed-data-add-three-features.csv"))
```

]

.panel[.panel-name[1]

**Split data**

```{r panel-chunk-1, echo = TRUE, eval = FALSE}
train_test_split <- initial_split(d, prop = .80)
data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train) # again, we will use resampling
```
]

.panel[.panel-name[2]

**Engineer features**

```{r panel-chunk-2, echo = TRUE, eval = FALSE}
my_rec <- recipe(code ~ ., data = data_train) %>% 
    step_normalize(all_numeric_predictors()) %>%
    step_nzv(all_predictors())
```
]

.panel[.panel-name[3]

**Specify recipe, model, and workflow**
 
```{r panel-chunk-3, echo = TRUE, eval = FALSE}
# specify model
my_mod <-
    rand_forest(mtry = tune(), # this specifies that we'll take steps later to tune the model
                min_n = tune()) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("regression")

# specify workflow
my_wf <-
    workflow() %>%
    add_model(my_mod) %>% 
    add_recipe(my_rec)
```
]
 
.panel[.panel-name[4]

**Fit model**

```{r panel-chunk-4, echo = TRUE, eval = FALSE}
# specify tuning grid
finalize(mtry(), data_train)
finalize(min_n(), data_train)

tree_grid <- grid_max_entropy(mtry(range(1, 18)),
                              min_n(range(2, 40)),
                              size = 10)

# fit model with tune_grid
fitted_model <- my_wf %>% 
    tune_grid(
        resamples = kfcv,
        grid = tree_grid,
        metrics = metric_set((roc_auc, accuracy, kap, sensitivity, specificity, precision))
```

]

.panel[.panel-name[5]

**Fit model (part 2)**

```{r panel-chunk-4b, echo = TRUE, eval = FALSE}
# examine best set of tuning parameters; repeat?
show_best(fitted_model, n = 10, metric = "accuracy")

# select best set of tuning parameters
best_tree <- fitted_model %>% select_best(metric = "accuracy")

# finalize workflow with best set of tuning parameters
final_wf <- my_wf %>% 
    finalize_workflow(best_tree)

final_fit <- final_wf %>% 
    last_fit(train_test_split, metrics = metric_set(roc_auc, accuracy, kap, sensitivity, specificity, precision))
```
]

.panel[.panel-name[6]

**Evaluate accuracy**

```{r panel-chunk-5, echo = TRUE, eval = FALSE}
# fit stats
final_fit %>%
    collect_metrics()

# variable importance plot
final_fit %>% 
    pluck(".workflow", 1) %>%   
    pull_workflow_fit() %>% 
    vip(num_features = 10)
```
]
]

---

# We'll next dive deeper

- **Case study**: We'll work through this code in much more depth to tune a model
- **Independent practice**: we'll try tuning an even more sophisticated model

---

class: clear, center

## .font130[.center[**Thank you!**]]

<br/>
.center[<img style="border-radius: 80%;" src="img/jr-cycling.jpeg" height="200px"/><br/>**Dr. Joshua Rosenberg**<br/><mailto:jmrosenberg@utk.edu>]
