---
title: 'Learning Lab 4 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

While the past three case studies have focused on cases for which we have coded data (in case studies 1 and 3, from the #NGSSchat data; in case study 2, from having students' end of course grades), sometimes, we do not have smething that we can consider to be coded data---or a dependent variable. Machine learning does offer us way forward in such cases, but not through the supervised machine learning methods we've been using until this point. Instead, we can use _unsupervised_ methods.

Our driving question is: What if we do not have training data? 

The goal is to estimate those groups, here through the process of Latent Profile Analysis. We follow the example of Commeford et al. (2022), who estimated profiles of college-levels instructors' teaching practices, specifically their teaching practices relating to active learning. They did so using Latent Profile Analysis -- and [the tidyLPA package in R](https://data-edu.github.io/tidyLPA/index.html) that we'll be using.

One other resource that may be helpful is the chapter by Pastor et al. (2007) on Latent Profile Analysis, or, as it is often abbreviated, LPA.


## Step 0: Loading and setting up

First, let's load the packages we'll use---the familiar {tidyverse} and the {tidyLPA package}.

```{r}
library(tidyverse)
library(tidyLPA)
```

Next, let's load the data.

```{r}
d <- read_csv("data/dat_csv_combine_final_full.csv")
```

We'll next select the variables we'll use for this analysis.

```{r}
d <- d %>% 
    select(AveCarelessness, AveKnow, AveCorrect = AveCorrect.x, AveResBored, 
           AveResEngcon, AveResConf, AveResFrust, AveResOfftask, 
           AveResGaming, NumActions) %>%
    janitor::clean_names()

d
```

## Step 1: Explore a range of solutions

We _could_ estimate a single solution, as in the following (run this code).

```{r}
d %>%
    estimate_profiles(3) %>% 
    plot_profiles()
```
Wait a minute - something's not right. Let's center and standardize all of the variables, first, to make this graph more interpretable.

This is a bit of a new technique, but we'll create a new function that we'll use to do this. There _is_ a built-in function in R, `scale()`, but it often causes problems because it returns a data frame (technically a matrix) with a single column, rather than simply a single column. In short, it doesn't work well in many cases, so we'll write our own version of it.

```{r}
scale_data <- function(x) {
    x = x - mean(x, na.rm = TRUE)
    x = x / sd(x, na.rm = TRUE)
    x
}
```

Then, we'll use the function as follows with the `mutate_all()` function, creating a scaled version of all of our variables and saving the results back to our data frame.

```{r}
d <- d %>%
    mutate_all(funs(scaled = scale_data)) %>% # using our function to scale all of the varialbves
    select(contains("_scaled")) # selecting only the scaled version
```

```{r}
d %>%
    estimate_profiles(3) %>% 
    plot_profiles()
```

Done, right? Not quite. Notice that above, we specified that the number of profiles was 3. How do we know that 3 is best? This is a key decision that we as the individual or group analyzing the data must make. Fortunately, there are tools to help us. Let's try the `compare_solutions()` function, which does just that. The `estimate_profiles()` function can, helpfully, estiamte more than one set of profiles at once --- here, those with 1 through 10 profiles.

```{r}
d %>%
    estimate_profiles(1:10) %>% 
    compare_solutions()
```

Often, we wish to look for a _change in the rate_ of the decrease of fit indices, with the BIC fit index being arguably the most important. Above, we can see that the rate of change appears to level off after around 4 "Classes" - or, profiles. Let's consider that solution as one that fits best (even though the utput suggests that the "best" model is one with 10 classes, as that may be over-fitting the data.

Let's interrogate that fourth solution next.

```{r}
our_solution <- d %>%
    estimate_profiles(4)

plot_profiles(our_solution, add_line = TRUE) +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) # this rotates the x-axis labels to make them easier to read
```

Typically, we'd name these solutions. Let's get started with one, the second:

- Profile 1: 
- Profile 2: Bored and off-task
- Profile 3:
- Profile 4:

You may wish to review the paper to interpret the other variables: https://educationaldatamining.org/EDM2014/uploads/procs2014/short%20papers/276_EDM-2014-Short.pdf

We still aren't done! Recall from the presentation how we discussed computational grounded theory. In this approach, an exploratory approach such as Latent Profile Analysis is carried out not as the end point, but as a first step in understanding the data. The second step involves examining the data with the output from the first step as a guide. In this step, you as the human can interrogate and make sense of the output of the first step.

```{r}
get_data(our_solution)
```

What we do we notice about the solutions? What might the model have missed? We'll explore these questions further in the independent practice.

### ðŸ§¶ Knit & Check âœ…

Congratulations - you've completed the Machine Learning Learning Lab 1 Guided Practice! Consider moving on to the independent practice next.