
---
title: "How to Cross‑Validate and Change the Model"
subtitle: "Code Along"
format:
  revealjs:
    slide-number: c/t
    progress: true
    chalkboard:
      buttons: false
    preview-links: auto
    logo: img/LASERLogoB.png
    theme: [default, css/laser.scss]
    width: 1920
    height: 1080
    margin: 0.05
    footer: <a href="https://www.go.ncsu.edu/laser-institute">go.ncsu.edu/laser-institute
---

## R Code

::: {.panel-tabset}

### 0. Loading & Setup

```{r}
#| eval: false
#| echo: true
library(tidyverse)
library(tidymodels)

pokemon <- read_csv("data/pokemon-data.csv")
pokemon %>% glimpse()
```

### 1. Create Cross‑Validation Folds

```{r}
#| eval: false
#| echo: true
set.seed(123)
pokemon_folds <- vfold_cv(pokemon, v = 5, strata = early_gen)
pokemon_folds
```

### 2. Specify Recipe

```{r}
#| eval: false
#| echo: true
pokemon_rec <- recipe(early_gen ~ height_m + weight_kg + hp, data = pokemon) %>% 
  step_mutate(early_gen = as.factor(early_gen))
```

### 3. Specify Models

```{r}
#| eval: false
#| echo: true
log_mod <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

boost_mod <- boost_tree(
    trees = 1000, 
    mtry = 3, 
    learn_rate = 0.05
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

### 4. Create Workflows

```{r}
#| eval: false
#| echo: true
log_wf <- workflow() %>% 
  add_recipe(pokemon_rec) %>% 
  add_model(log_mod)

boost_wf <- workflow() %>% 
  add_recipe(pokemon_rec) %>% 
  add_model(boost_mod)
```

### 5. Fit Resamples & Collect Metrics

```{r}
#| eval: false
#| echo: true
log_res <- fit_resamples(
  log_wf,
  resamples = pokemon_folds,
  metrics   = metric_set(accuracy)
)

boost_res <- fit_resamples(
  boost_wf,
  resamples = pokemon_folds,
  metrics   = metric_set(accuracy)
)

bind_rows(
  collect_metrics(log_res)   %>% mutate(model = "Logistic Regression"),
  collect_metrics(boost_res) %>% mutate(model = "XGBoost")
) %>% 
  select(model, mean, std_err)
```

### 6. Variable Importance (VIP) for XGBoost

```{r}
#| eval: false
#| echo: true
library(vip)

boost_fit <- fit(boost_wf, data = pokemon)

boost_fit %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```

:::

## Code‑along: Python

```{python}
#| eval: false
#| echo: true
import pandas as pd
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import shap

pokemon = pd.read_csv("data/pokemon-data.csv")

# Features and target
X = pokemon[["height_m", "weight_kg", "hp"]]
y = pokemon["early_gen"]

# Cross‑validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)

log_model = LogisticRegression(max_iter=1000)

xgb_model = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=3,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="binary:logistic",
    eval_metric="logloss",
    random_state=123
)

log_scores = cross_val_score(log_model, X, y, cv=kf, scoring='accuracy')
xgb_scores = cross_val_score(xgb_model,  X, y, cv=kf, scoring='accuracy')

print(f"Logistic Regression accuracy: {log_scores.mean():.3f}")
print(f"XGBoost accuracy:             {xgb_scores.mean():.3f}")

# Fit the XGB on the full data for feature importance
xgb_model.fit(X, y)
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X)

# Bar plot of feature importance
shap.summary_plot(shap_values, X, plot_type="bar", max_display=10)
```
