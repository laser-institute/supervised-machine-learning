---
title: 'Modeling Interactions Data with Boosted Trees'
subtitle: "Case Study"
author: "LASER Institute"
date: today
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
# bibliography: lit/references.bib
resource_files:
- img/tidymodels.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

## 1. PREPARE

After interpreting our last model, it is easy to think we can do a little better. But, how? In this module, we attempt to improve our predictive performance by **switching from a random‑forest model to a boosted‑tree model (using *xgboost*)** and by carrying out some targeted feature engineering.

Boosted trees (gradient boosting) build a *sequence* of small trees, each one focusing on the records the previous trees struggled with. When tuned carefully (small learning rate, many trees) the ensemble can outperform bagged methods such as random forests.

Feature engineering is a rich topic in machine learning research, including in the learning analytics and educational data‑mining communities.

\[...\]

*(narrative unchanged until the section where the model type is first mentioned; replace “random forest” with “boosted tree” throughout)*

## 2. WRANGLE

### Step 0: Loading and setting up

First, load the packages we'll use—`tidyverse` and several others focused on modeling.

#### **👉 Your Turn ⤵**

Add to the chunk below code to load:

-   **tidyverse**, **janitor**, **tidymodels**
-   **xgboost** (boosted‑tree engine)
-   **vip** (variable importance plots)

```{r}
# Example
library(tidyverse)
library(janitor)
library(tidymodels)
library(xgboost)   # engine used by boost_tree()
library(vip)
```

*(rest of WRANGLE section unchanged)*

## 4. MODEL

### Step 1. Split data

*(instructions unchanged)*

### Step 2: Engineer features and write the recipe

*(instructions unchanged)*

### Step 3: Specify the model and workflow

Here we **swap in an XGBoost model**:

```{r}
# specify model
my_mod <-
  boost_tree(learn_rate = 0.05, trees = 1000, tree_depth = 4) %>%  # boosted tree
  set_engine("xgboost", importance = "gain") %>%                    # xgboost engine
  set_mode("classification")

# specify workflow
my_wf <-
  workflow() %>% 
  add_model(my_mod) %>% 
  add_recipe(my_rec)
```

> **Why those defaults?**\
> - **trees**: many small steps (1 000)\
> - **learn_rate**: 0.05 (smaller → safer, needs more trees)\
> - **tree_depth**: shallow (4) to avoid over‑fitting

### Step 4: Fit model with cross‑validation

```{r}
class_metrics <- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)

fitted_model_resamples <- fit_resamples(
  my_wf,
  resamples = vfcv,        # 10‑fold CV by default
  metrics   = class_metrics
)
```

Collect metrics:

```{r}
collect_metrics(fitted_model_resamples)
```

### Step 5: Final fit on the test set

```{r}
final_fit <- last_fit(my_wf, train_test_split, metrics = class_metrics)

final_fit %>%
  collect_metrics()
```

## 5. COMMUNICATE

Boosted trees can also deliver variable‑importance scores. Use `vip()` on the fitted xgboost object:

```{r}
final_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 10)
```

*(prompts for reflections unchanged, just replace “random forest” with “boosted tree” where needed)*

### 🧶 Knit & Check ✅

Congratulations—you have created an XGBoost model, compared its performance with cross‑validation, and interpreted the top features!
