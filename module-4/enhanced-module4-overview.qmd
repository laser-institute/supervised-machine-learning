---
title: "Predicting Better with Random Forests"
subtitle: "Conceptual Overview"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: img/LASERLogoB.png
    theme: [default, css/laser.scss]
    width: 1920
    height: 1080
    margin: 0.05
    footer: <a href=https://www.go.ncsu.edu/laser-institute>go.ncsu.edu/laser-institute
---

```{r}
#| echo: false
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

# Purpose and Agenda

Having fit and interpreted our machine learning models in previous modules, how do we make them better? That's the focus of this learning lab. We'll explore three advanced concepts that can significantly improve our predictions: sophisticated feature engineering, cross-validation, and the random forest algorithm. We'll continue using the IPEDS data to predict institutional graduation rates, but with more powerful techniques.

::: {.notes}
This module brings together everything we've learned and takes it to the next level. While the first three modules built a strong foundation, here we'll add significant complexity to create more powerful predictive models.
:::

## What we'll do in this presentation

-   Discussion 1
-   Key Concept: The complete SML workflow
-   Key Concept: Advanced feature engineering
-   Key Concept: Resampling and cross-validation
-   Key Concept: The Random Forest algorithm
-   Discussion 2
-   Introduction to the other parts of this learning lab

# Discussion 1

::: panel-tabset
## Reflecting

-   Now that we've worked with the IPEDS data through three modules, what additional information or transformations do you think would help us better predict institutional graduation rates?

## Applying

-   Have you encountered a situation where simple models weren't capturing important patterns in your data? What approaches did you try (or might you try) to address this?
:::

# Key Concept #1: The complete SML workflow

## Our SML journey culminates

<div style="display: flex; justify-content: space-between;">
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 1</h3>
  <p>Understanding prediction vs. explanation</p>
  <p><i>Same model, different goals</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 2</h3>
  <p>Implementing training/testing workflows</p>
  <p><i>Avoiding overfitting with proper evaluation</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #f0f0f0; margin: 5px; border-radius: 10px;">
  <h3>Module 3</h3>
  <p>Evaluating model performance rigorously</p>
  <p><i>Beyond simple accuracy</i></p>
</div>
<div style="flex: 1; padding: 10px; text-align: center; background-color: #4a86e8; color: white; margin: 5px; border-radius: 10px;">
  <h3>Module 4</h3>
  <p>Advanced feature engineering & algorithms</p>
  <p><i>Maximizing predictive power</i></p>
</div>
</div>

## The complete SML workflow

```{r, echo=FALSE, fig.align="center"}
workflow_data <- data.frame(
  Stage = c("Data Preparation", "Feature Engineering", "Model Selection", "Cross-Validation", "Hyperparameter Tuning", "Final Evaluation"),
  Description = c(
    "Split data, handle missing values, encode categorical variables",
    "Create new variables, transform existing ones, select features",
    "Choose appropriate algorithm based on data and objectives",
    "Validate model on multiple data subsets to ensure robustness",
    "Optimize model parameters for best performance",
    "Assess final model on held-out test data"
  ),
  Status = c("✓ Modules 1-2", "➡️ Module 4", "➡️ Module 4", "➡️ Module 4", "➡️ Module 4", "✓ Modules 2-3")
)

knitr::kable(workflow_data)
```

## Building on our IPEDS foundation

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5.5, fig.align="center"}
# Load sample IPEDS data for demonstration
set.seed(123)
n <- 200
ipeds_sample <- data.frame(
  total_enroll = runif(n, 1000, 30000),
  pct_admitted = runif(n, 30, 100),
  tuition_fees = runif(n, 10000, 50000),
  percent_fin_aid = runif(n, 40, 95),
  avg_salary = runif(n, 50000, 120000),
  n_bach = runif(n, 500, 5000)
)

# Create derived variables
ipeds_sample$bach_ratio <- ipeds_sample$n_bach / ipeds_sample$total_enroll
ipeds_sample$tuition_per_student <- ipeds_sample$tuition_fees / ipeds_sample$total_enroll * 1000
ipeds_sample$selectivity_tier <- cut(ipeds_sample$pct_admitted, breaks=c(0, 25, 50, 75, 100), 
                                     labels=c("Very Selective", "Selective", "Moderately Selective", "Less Selective"))

# Create the true probabilities and graduation rate outcome
ipeds_sample$grad_prob <- with(ipeds_sample, {
  0.3 + 0.1*scale(bach_ratio) - 0.05*scale(pct_admitted) + 0.15*scale(tuition_fees) + 
    0.1*scale(percent_fin_aid) + 0.2*scale(avg_salary) + 0.1*scale(tuition_per_student) + rnorm(n, 0, 0.1)
})
ipeds_sample$grad_prob <- pmin(pmax(ipeds_sample$grad_prob, 0), 1)
ipeds_sample$good_grad_rate <- as.factor(ifelse(ipeds_sample$grad_prob > 0.5, 1, 0))

# Create plots comparing basic vs. derived features
library(gridExtra)

# Plot 1: Basic features
p1 <- ggplot(ipeds_sample, aes(x=pct_admitted, y=avg_salary, color=good_grad_rate)) +
  geom_point(alpha=0.7) +
  scale_color_manual(values=c("0"="#F1948A", "1"="#7DCEA0"), 
                     labels=c("Poor", "Good")) +
  theme_minimal() +
  labs(title="Basic Features",
       x="Admission Rate (%)",
       y="Average Faculty Salary ($)",
       color="Graduation Rate") +
  theme(legend.position="bottom")

# Plot 2: Derived features
p2 <- ggplot(ipeds_sample, aes(x=bach_ratio, y=tuition_per_student, color=good_grad_rate)) +
  geom_point(alpha=0.7) +
  scale_color_manual(values=c("0"="#F1948A", "1"="#7DCEA0"), 
                     labels=c("Poor", "Good")) +
  theme_minimal() +
  labs(title="Derived Features",
       x="Bachelor's Completion Ratio",
       y="Tuition per Student ($)",
       color="Graduation Rate") +
  theme(legend.position="bottom")

grid.arrange(p1, p2, ncol=2)
```

# Key Concept #2: Advanced feature engineering

## What is feature engineering?

::: {.incremental}
- **Feature engineering** is the art and science of transforming raw data into inputs that better represent the underlying patterns
  
- It's often the difference between a mediocre model and an excellent one

- For IPEDS data, this could mean:
  - Creating meaningful ratios (e.g., faculty-to-student ratio)
  - Developing composite indicators (e.g., financial health index)
  - Encoding geographical information (e.g., urban/rural, regional indicators)
  - Capturing interaction effects between variables
:::

## Feature engineering approaches

```{r, echo=FALSE, fig.align="center"}
fe_approaches <- data.frame(
  Approach = c("Transformation", "Interaction", "Aggregation", "Decomposition", "Encoding", "Domain-Specific"),
  Description = c(
    "Apply mathematical functions to change variable distribution or scale",
    "Create new variables that capture relationships between two or more variables",
    "Combine multiple variables into meaningful summary metrics",
    "Break down complex variables into component parts",
    "Convert categorical variables into numeric representations",
    "Create variables based on subject-matter expertise"
  ),
  IPEDS_Example = c(
    "Log-transform enrollment to handle large size differences",
    "Interaction between selectivity and financial aid (might affect different income groups differently)",
    "Create an institutional resources index combining multiple financial variables",
    "Break down total degrees awarded by field or level",
    "Create regional indicators based on state location",
    "Calculate retention-to-graduation conversion rate based on educational research"
  )
)

knitr::kable(fe_approaches)
```

## Mathematical transformations

Common transformations and when to use them with IPEDS data:

::: {.incremental}
- **Logarithmic transformation**: 
  - When variables have right-skewed distributions
  - Example: Enrollment, endowment, institutional budget
  - Benefits: Reduces impact of outliers, better captures percentage changes

- **Standardization (z-scores)**:
  - When variables are on different scales
  - Example: Comparing tuition, faculty salaries, and financial aid
  - Benefits: Puts all numeric variables on same scale, improves model convergence

- **Normalization (min-max scaling)**:
  - When you need values between 0 and 1 or want to preserve distributional shape
  - Example: Creating index variables from multiple indicators
  - Benefits: Maintains original distribution shape, constrains range
:::

## Creating domain-informed features

For IPEDS institutional data, consider these meaningful derived variables:

::: {.incremental}
- **Resource intensity**: Faculty salary × faculty-student ratio
  - Captures overall instructional investment per student

- **Affordability index**: (Tuition - average financial aid) / median state income
  - Measures relative financial accessibility

- **Academic environment score**: Composite of admission rate, student-faculty ratio, and research output
  - Reflects academic quality and competitiveness

- **Student support ratio**: Student services expenditure / total enrollment
  - Indicates investment in student success infrastructure

- **Geographic context**: Rurality index, economic opportunity measures, regional education levels
  - Captures external factors that influence student outcomes
:::

## Feature selection and importance

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5.5, fig.align="center"}
# Create a simulated feature importance plot
features <- c("Bachelor's completion ratio", "Average faculty salary", "Tuition & fees", 
              "Financial aid percentage", "Admission rate", "Student support ratio", 
              "Research expenditure", "Endowment per student", "Student-faculty ratio",
              "Regional education level")

importance <- c(100, 87, 76, 65, 54, 43, 32, 21, 18, 12)

importance_df <- data.frame(
  Feature = factor(features, levels = features[order(importance, decreasing=TRUE)]),
  Importance = importance,
  Category = c("Derived", "Basic", "Basic", "Basic", "Basic", "Derived", "Basic", "Derived", "Derived", "Derived")
)

ggplot(importance_df, aes(x=reorder(Feature, Importance), y=Importance, fill=Category)) +
  geom_bar(stat="identity") +
  coord_flip() +
  scale_fill_manual(values=c("Basic"="#5DADE2", "Derived"="#F4D03F")) +
  theme_minimal(base_size=14) +
  labs(title="Feature Importance for Graduation Rate Prediction",
       subtitle="Note how derived features often outrank basic features",
       x="",
       y="Relative Importance") +
  theme(legend.position="bottom",
        plot.title=element_text(size=16, face="bold"))
```

# Key Concept #3: Resampling and cross-validation

## The need for cross-validation

::: {.incremental}
- **Problem**: If we repeatedly use the same test set to evaluate model improvements, we risk overfitting to the test set
  
- **Solution**: Cross-validation provides a more robust estimate of model performance by:
  - Testing on multiple subsets of data
  - Averaging performance across multiple validation sets
  - Allowing for feature and model selection without "spending" the test set
  
- **Benefits for IPEDS analysis**:
  - More reliable assessment of how our model will generalize to new institutions
  - Better feature selection for institutional graduation rate prediction
  - Ability to compare multiple modeling approaches objectively
:::

## K-fold cross-validation

```{r, echo=FALSE, fig.width=10, fig.height=5.5, fig.align="center"}
# Create a visualization of K-fold cross-validation
set.seed(123)
n_folds <- 5
n_samples <- 100
cv_data <- expand.grid(Sample=1:n_samples, Fold=1:n_folds)
cv_data$Role <- "Training"

# Assign test samples for each fold
for(i in 1:n_folds) {
  test_indices <- seq(i, n_samples, by=n_folds)
  cv_data$Role[cv_data$Fold == i & cv_data$Sample %in% test_indices] <- "Validation"
}

# Create the plot
ggplot(cv_data, aes(x=Sample, y=Fold, fill=Role)) +
  geom_tile(color="white", size=0.5) +
  scale_fill_manual(values=c("Training"="#5DADE2", "Validation"="#F4D03F")) +
  theme_minimal(base_size=14) +
  labs(title="5-Fold Cross-Validation",
       subtitle="Each institution appears in the validation set exactly once",
       x="Institution",
       y="Fold") +
  theme(legend.position="bottom",
        plot.title=element_text(size=16, face="bold"),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  annotate("text", x=50, y=1.5, 
           label="In each fold, we train on 80% of the data\nand validate on the remaining 20%", 
           size=5, hjust=0.5) +
  annotate("text", x=50, y=4.5, 
           label="Final performance is averaged\nacross all 5 validation sets", 
           size=5, hjust=0.5)
```

## Process and implementation

Implementing cross-validation with IPEDS data:

::: {.incremental}
1. **Split data**: Divide institutions into k equal-sized folds
   
2. **Iterate k times**:
   - Use k-1 folds for training
   - Use the remaining fold for validation
   - Calculate performance metrics on the validation fold
   
3. **Average results**: Calculate the mean performance across all k iterations
   
4. **Select features/models**: Choose the combination with the best average performance
   
5. **Final evaluation**: Only after all model decisions are made, evaluate on the held-out test set
:::

## Avoiding bias in model development

```{r, echo=FALSE, fig.align="center"}
bias_comparison <- data.frame(
  Approach = c("No Validation (One Dataset)", "Train/Test Split", "Cross-Validation"),
  Risk = c("Extreme overfitting, misleadingly high performance estimates", 
           "Feature selection may overfit to single test set", 
           "More robust performance estimation, reduced risk of overfitting"),
  Appropriate_For = c("Never appropriate for predictive modeling", 
                      "Simple models with few features", 
                      "Complex models, feature engineering, model selection")
)

knitr::kable(bias_comparison)
```

# Key Concept #4: The Random Forest algorithm

## From decision trees to random forests

::: {.incremental}
- **Decision trees** create a flowchart-like structure of if-then rules
  - Simple to understand but prone to overfitting
  
- **Random forests** combine many decision trees to create a more robust model:
  - Each tree is built using a random subset of data
  - Each split considers a random subset of features
  - Final prediction is determined by majority vote (classification) or average (regression)
  
- **Benefits for IPEDS analysis**:
  - Can capture nonlinear relationships between institutional characteristics and graduation rates
  - Naturally handles interactions between variables
  - Provides feature importance metrics
:::

## How random forests work

```{r, echo=FALSE, fig.width=10, fig.height=5.5, fig.align="center"}
# Create a visual representation of random forest
par(mfrow=c(1,1), mar=c(0,0,0,0))
plot(0, 0, type="n", xlim=c(0,10), ylim=c(0,7), xaxt="n", yaxt="n", xlab="", ylab="", bty="n")

# Draw the data
points(1, 6, pch=16, col="black", cex=2)
text(1, 6.5, "Training Data", cex=1.2)

# Draw arrows to trees
arrows(1.5, 6, 2.5, 6.8, length=0.15)
arrows(1.5, 6, 2.5, 6, length=0.15)
arrows(1.5, 6, 2.5, 5.2, length=0.15)

# Draw trees
symbols(3, 6.8, circles=0.5, add=TRUE, inches=FALSE)
text(3, 6.8, "Tree 1", cex=1)
text(3, 6.5, "Random\nSubset", cex=0.7)

symbols(3, 6, circles=0.5, add=TRUE, inches=FALSE)
text(3, 6, "Tree 2", cex=1)
text(3, 5.7, "Random\nSubset", cex=0.7)

symbols(3, 5.2, circles=0.5, add=TRUE, inches=FALSE)
text(3, 5.2, "Tree 3", cex=1)
text(3, 4.9, "Random\nSubset", cex=0.7)

text(3, 4.2, "...", cex=1.5)

symbols(3, 3.5, circles=0.5, add=TRUE, inches=FALSE)
text(3, 3.5, "Tree n", cex=1)
text(3, 3.2, "Random\nSubset", cex=0.7)

# Draw arrows to predictions
arrows(3.5, 6.8, 4.5, 6.8, length=0.15)
arrows(3.5, 6, 4.5, 6, length=0.15)
arrows(3.5, 5.2, 4.5, 5.2, length=0.15)
arrows(3.5, 3.5, 4.5, 3.5, length=0.15)

# Draw predictions
symbols(5, 6.8, circles=0.5, add=TRUE, inches=FALSE)
text(5, 6.8, "Pred 1", cex=1)

symbols(5, 6, circles=0.5, add=TRUE, inches=FALSE)
text(5, 6, "Pred 2", cex=1)

symbols(5, 5.2, circles=0.5, add=TRUE, inches=FALSE)
text(5, 5.2, "Pred 3", cex=1)

text(5, 4.2, "...", cex=1.5)

symbols(5, 3.5, circles=0.5, add=TRUE, inches=FALSE)
text(5, 3.5, "Pred n", cex=1)

# Draw arrow to final prediction
arrows(5.5, 6.8, 6.5, 5.5, length=0.15)
arrows(5.5, 6, 6.5, 5.5, length=0.15)
arrows(5.5, 5.2, 6.5, 5.5, length=0.15)
arrows(5.5, 3.5, 6.5, 5.5, length=0.15)

# Draw final prediction
symbols(7, 5.5, circles=0.8, add=TRUE, inches=FALSE)
text(7, 5.5, "Aggregate\nPredictions", cex=1.2)

# Add new data
points(5, 1.5, pch=16, col="black", cex=2)
text(5, 2, "New Institution", cex=1.2)
arrows(5, 2.2, 7, 4.7, length=0.15)

# Draw final prediction
symbols(9, 3.5, circles=0.8, add=TRUE, inches=FALSE)
text(9, 3.5, "Final\nPrediction", cex=1.2)
arrows(7.8, 5.1, 8.5, 4, length=0.15)
arrows(5.8, 1.5, 8.3, 3, length=0.15)

# Add titles and labels
text(1.5, 1, "1. Create multiple decision trees\n   from random subsets", cex=1, adj=0)
text(5, 0.5, "2. Make predictions with\n   each tree", cex=1, adj=0)
text(9, 2, "3. Combine predictions\n   (voting/averaging)", cex=1, adj=0.5)
```

## Why random forests excel with IPEDS data

Random forests offer several advantages for predicting institutional graduation rates:

::: {.incremental}
- **Handle complexity**: Capture the multifaceted nature of institutional success factors
  
- **Resistance to overfitting**: More stable predictions across different institutional types
  
- **Feature importance**: Identify which institutional characteristics most strongly predict graduation rates
  
- **Missing data tolerance**: Can handle some missingness in the IPEDS variables
  
- **No distributional assumptions**: Don't require normally distributed variables (unlike some statistical models)
  
- **Capture non-linear relationships**: Education outcomes often have threshold effects and interactions
:::

## Random forest tuning parameters

```{r, echo=FALSE, fig.align="center"}
rf_params <- data.frame(
  Parameter = c("Number of trees (ntree)", "Variables per split (mtry)", "Minimum node size", "Maximum depth"),
  Description = c(
    "How many decision trees to build in the forest",
    "Number of variables randomly sampled as candidates at each split",
    "Minimum number of observations required in a leaf node",
    "The maximum depth allowed for each tree (complexity control)"
  ),
  Typical_Range = c(
    "100-1000 (more is usually better but diminishing returns)",
    "For classification: sqrt(number of predictors); For regression: number of predictors/3",
    "1-5 for classification, 5-10 for regression",
    "No limit by default, but can be restricted to prevent overfitting"
  ),
  Impact = c(
    "More trees → more stable predictions but slower training",
    "Higher values → more like bagging, lower values → more diversity between trees",
    "Larger values → simpler trees, less overfitting but may miss patterns",
    "Restricting depth → less complex trees, reduced overfitting"
  )
)

knitr::kable(rf_params)
```

## Model interpretation with variable importance

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=5, fig.align="center"}
# Create partial dependence plots for two key features
set.seed(123)
n <- 100

# Create data for partial dependence plots
faculty_salary <- seq(50000, 120000, length.out=n)
completion_ratio <- seq(0.05, 0.4, length.out=n)

# Create probabilities
faculty_probs <- plogis(-5 + faculty_salary * 8e-5)
completion_probs <- plogis(-2 + completion_ratio * 15)

# Combine into data frames
pdp1 <- data.frame(
  Feature_Value = faculty_salary,
  Grad_Probability = faculty_probs
)

pdp2 <- data.frame(
  Feature_Value = completion_ratio,
  Grad_Probability = completion_probs
)

# Create plots
p1 <- ggplot(pdp1, aes(x=Feature_Value, y=Grad_Probability)) +
  geom_line(size=1.5, color="#5DADE2") +
  theme_minimal(base_size=14) +
  labs(title="Average Faculty Salary",
       x="Salary ($)",
       y="Probability of Good Graduation Rate") +
  theme(plot.title=element_text(hjust=0.5, size=16)) +
  scale_x_continuous(labels=scales::dollar_format()) +
  scale_y_continuous(limits=c(0, 1))

p2 <- ggplot(pdp2, aes(x=Feature_Value, y=Grad_Probability)) +
  geom_line(size=1.5, color="#5DADE2") +
  theme_minimal(base_size=14) +
  labs(title="Bachelor's Completion Ratio",
       x="Completion Ratio",
       y="Probability of Good Graduation Rate") +
  theme(plot.title=element_text(hjust=0.5, size=16)) +
  scale_y_continuous(limits=c(0, 1))

gridExtra::grid.arrange(p1, p2, ncol=2,
                        top=grid::textGrob("Partial Dependence Plots: Impact of Key Features", 
                                          gp=grid::gpar(fontsize=18, fontface="bold")))
```

# Discussion 2

::: panel-tabset
## Reflecting

-   How might the advanced techniques we've discussed in this module (feature engineering, cross-validation, random forests) help address limitations you've encountered in previous data analysis?

## Applying

-   Think about your own research. What specific feature engineering techniques would be most valuable for your particular research questions and datasets?
:::

# Introduction to the other parts of this learning lab

::: panel-tabset
## Readings

> Baker, R. S., Esbenshade, L., Vitale, J., & Karumbaiah, S. (2023). Using demographic data as predictor variables: A questionable choice. *Journal of Educational Data Mining, 15*(2), 22-52.

> Bosch, N. (2021). AutoML feature engineering for student modeling yields high accuracy, but limited interpretability. Journal of Educational Data Mining, 13(2), 55-79.

> Rodriguez, F., Lee, H. R., Rutherford, T., Fischer, C., Potma, E., & Warschauer, M. (2021, April). Using clickstream data mining techniques to understand and support first-generation college students in an online chemistry course. In LAK21: 11th International Learning Analytics and Knowledge Conference (pp. 313-322).

## Case Study

-   Creating advanced features from IPEDS institutional data
-   Implementing cross-validation to guard against overfitting
-   Applying random forest models to capture complex patterns in graduation rates
-   Interpreting variable importance to understand key predictors
-   Comparing performance against our previous models

## Badge

-   Considering the ethical implications of feature selection
-   Adding domain-specific features derived from education research
-   Evaluating the trade-offs between model complexity and interpretability
-   Designing an end-to-end workflow for predicting graduation rates
:::

# *fin*

-   *Dr. Joshua Rosenberg* (jmrosenberg\@utk.edu; https://joshuamrosenberg.com)

[General troubleshooting tips for R and RStudio](https://docs.google.com/document/d/14Jc-KG3m5k1BvyKWqw7KmDD21IugU5nV5edfJkZyspY/edit)