---
title: 'Modeling Interactions Data with Boosted Trees'
subtitle: "Case Study"
author: "LASER Institute"
date: today
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
# bibliography: lit/references.bib
resource_files:
- img/tidymodels.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

## 1. PREPARE

After interpreting our last model, it is easy to think we can do a little better. But, how? In this module, we attempt to improve our predictive performance by **switching from a randomâ€‘forest model to a boostedâ€‘tree model (using *xgboost*)** and by carrying out some targeted feature engineering.

Boosted trees (gradient boosting) build a *sequence* of small trees, each one focusing on the records the previous trees struggled with. When tuned carefully (small learning rate, many trees) the ensemble can outperform bagged methods such as random forests.

Feature engineering is a rich topic in machine learning research, including in the learning analytics and educational dataâ€‘mining communities.

\[...\]

*(narrative unchanged until the section where the model type is first mentioned; replace â€œrandom forestâ€ with â€œboosted treeâ€ throughout)*

## 2. WRANGLE

### StepÂ 0: Loading and setting up

First, load the packages we'll useâ€”`tidyverse` and several others focused on modeling.

#### **ğŸ‘‰Â Your TurnÂ â¤µ**

Add to the chunk below code to load:

-   **tidyverse**, **janitor**, **tidymodels**
-   **xgboost** (boostedâ€‘tree engine)
-   **vip** (variable importance plots)

```{r}
# Example
library(tidyverse)
library(janitor)
library(tidymodels)
library(xgboost)   # engine used by boost_tree()
library(vip)
```

*(rest of WRANGLE section unchanged)*

## 4. MODEL

### StepÂ 1. Split data

*(instructions unchanged)*

### StepÂ 2: Engineer features and write the recipe

*(instructions unchanged)*

### StepÂ 3: Specify the model and workflow

Here we **swap in an XGBoost model**:

```{r}
# specify model
my_mod <-
  boost_tree(learn_rate = 0.05, trees = 1000, tree_depth = 4) %>%  # boosted tree
  set_engine("xgboost", importance = "gain") %>%                    # xgboost engine
  set_mode("classification")

# specify workflow
my_wf <-
  workflow() %>% 
  add_model(my_mod) %>% 
  add_recipe(my_rec)
```

> **Why those defaults?**\
> - **trees**: many small steps (1â€¯000)\
> - **learn_rate**: 0.05 (smaller â†’ safer, needs more trees)\
> - **tree_depth**: shallow (4) to avoid overâ€‘fitting

### StepÂ 4: Fit model with crossâ€‘validation

```{r}
class_metrics <- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)

fitted_model_resamples <- fit_resamples(
  my_wf,
  resamples = vfcv,        # 10â€‘fold CV by default
  metrics   = class_metrics
)
```

Collect metrics:

```{r}
collect_metrics(fitted_model_resamples)
```

### StepÂ 5: Final fit on the test set

```{r}
final_fit <- last_fit(my_wf, train_test_split, metrics = class_metrics)

final_fit %>%
  collect_metrics()
```

## 5. COMMUNICATE

Boosted trees can also deliver variableâ€‘importance scores. Use `vip()` on the fitted xgboost object:

```{r}
final_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 10)
```

*(prompts for reflections unchanged, just replace â€œrandom forestâ€ with â€œboosted treeâ€ where needed)*

### ğŸ§¶Â Knit & CheckÂ âœ…

Congratulationsâ€”you have created an XGBoost model, compared its performance with crossâ€‘validation, and interpreted the top features!
